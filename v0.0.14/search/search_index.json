{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent is focused on developing a consistent way to deploy and manage Kubernetes clusters at scale. One way to think of k0rdent is as a \"super control plane\" designed to manage other Kubernetes control planes. Another way to think of k0rdent is as a platform for Platform Engineering. If you are building an internal developer platform (IDP), need a way to manage Kubernetes clusters at scale in a centralized place, create Golden Paths, etc. k0rdent is a great way to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do so. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a-vs-hmc-naming","title":"k0rdent vs Project 2A vs HMC naming","text":"<p>k0rdent is the official name of an internal Mirantis project that was originally codenamed \"Project 2A\". During our initial skunkworks-style 3-month MVP push, the code was put into a repository named HMC, which stood for \"Hybrid Multi-Cluster Controller\". What is HMC became k0rdent Cluster Manager (kcm), but it may be a little confusing because the overall project was still called \"Project 2A\" or even \"HMC\" at times.</p> <p>So, to be clear, here are the names and components:</p> <ul> <li>k0rdent: the overall project name</li> <li>k0rdent Cluster Manager (kcm)</li> <li>k0rdent State Manager (ksm)<ul> <li>This is currently rolled into kcm, but will be split out in the   future</li> <li>ksm leverages Project Sveltos   for certain functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (kof)</li> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and kcm   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (kcm)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (ksm)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations and more.</p> </li> <li> <p>k0rdent Observability and FinOps (kof)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to development process and developer specific notes located in the main repository.</p>"},{"location":"admin-install/","title":"Installation","text":"<p>k0rdent installation is pretty straightforward. As discussed in Architecture, k0rdent runs in a management Kubernetes cluster, so installation involves creating that cluster and adding k0rdent to it. Creating managed clusters is handled by k0rdent itself. This section of the guide covers:</p> <ul> <li>Creating the management cluster</li> <li>Preparing k0rdent to create managed clusters on multiple providers</li> <li>Creating and destroying managed clusters</li> <li>Creating, testing, and approving cluster templates for use by others</li> <li>Creating, testing, and approving service templates for use by others</li> <li>Extended management configuration</li> <li>Upgrading k0rdent</li> </ul>"},{"location":"admin-install/#prerequisites","title":"Prerequisites","text":"<p>Before you can install k0rdent, you'll need to create the substrate in which it will run.</p>"},{"location":"admin-install/#create-the-management-cluster-server","title":"Create the management cluster server","text":"<p>The k0rdent management cluster can run on a single Ubuntu server. Where that server resides isn't important, as long as it can reach the providers on which you want to run managed clusters. For example, you can run k0rdent in a management cluster on a VMware VM, and use it to create clusters on AWS, Azure, and so on. This platform independence makes it easier to manage multi-cloud installations. You can even run the management cluster on a (suitably performant) laptop.</p> <p>What's more, while the management cluster can perform operations on the managed clusters, these managed clusters are essentially independent of it. Once they've been created, they are simply Kubernetes clusters; if the Management Cluster goes away, they will continue to function unimpeded.</p> <p>For example, you can create and configure a series of edge clusters, then shut down the management cluster until you need to make changes at the edge.</p> <p>The management cluster server should meet the following minimum requirements:</p> <p>[[[[ INSERT MINIMUM REQUIREMENTS HERE. Include kubectl and Helm. ]]]]</p> <p>Once this server is in place, move on to creating the actual management cluster.</p>"},{"location":"admin-install/#create-and-prepare-a-kubernetes-cluster","title":"Create and prepare a Kubernetes cluster","text":"<p>If you already have a Kubernetes cluster into which you want to install k0rdent, skip to the next step. Otherwise, follow these steps to install and prepare the management cluster:</p> <ol> <li> <p>Install Docker</p> <p>Kubernetes, of course, requires Docker. You can find the full instructions for installing Docker at the Docker site, but here's a simplified version. </p> <p>First uninstall older, potentially conflicting versions:</p> <pre><code>for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre> <p>Then use the Docker convenience script to deploy Docker:</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n</code></pre> <p>It's not necessary to prepare Docker for non-root usage.</p> </li> <li> <p>Deploy a Kubernetes cluster</p> <p>The next step is to create the actual cluster itself. Note that the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see a single node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment varable so <code>kubectl</code> knows where to find it.</p> <p><pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install k0rdent is through its Helm chart, so let's get Helm installed:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin-install/#install-k0rdent","title":"Install k0rdent","text":"<p>The actual management cluster is a Kubernetes cluster with the k0rdent application installed. The simplest way to install k0rdent is through its Helm chart:</p> <p><pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/mirantis/hmc/charts/hmc:0.0.3\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: hmc\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: hmc-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p> <p>While the installation process appears to be done at this point, it's not; multiple pods and templates are being created in the background, and the entire process takes a few minutes.  </p> <p>To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 15:</p> <p><pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre></p> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 9 pods are running in the <code>projectsveltos</code> namespace:</p> <p><pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre></p> <p>If any of these pods are missing, simply give k0rdent more time. If there's a problem, you'll see pods crashing and restarting, and you can review the logs at [[[ ADD LOG LOCATION HERE ]]]. As long as that's not happening, you just need to wait a few minutes.</p> <p>Next verify whether the kcm templates have been successfully installed and reconciled.  Start with the <code>ProviderTemplate</code> objects:</p> <p>[[[ ADD ACTUAL VERSION NUMBERS TO THE OUTPUT ]]]</p> <p><pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre></p> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <p>[[[ ADD ACTUAL VERSION NUMBERS TO THE OUTPUT ]]]</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre></p> <p>Finally, make sure the <code>ServiceTemplate</code> objects are installed and valid:</p> <p><pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre></p>"},{"location":"admin-install/#air-gapped-installation","title":"Air-gapped Installation","text":"<p>WARNING: Currently only the vSphere infrastructure provider supports full air-gapped installation.</p>"},{"location":"admin-install/#prerequisites_1","title":"Prerequisites","text":"<p>In order to install k0rdent in an air-gapped environment, you need will need the following:</p> <ul> <li>A k0s cluster installed on vSphere. This cluster will act as the management cluster. (While k0rdent works with any certified Kubernetes distribution, k0s implements an OCI image bundle watcher that enables k0s to easily utilize a bundle of management cluster images. You can follow the Airgapped Installation documentation to create this cluster if necessary.</li> <li>The <code>KUBECONFIG</code> of the management cluster.</li> <li> <p>A registry that is accessible from the airgapped hosts in which to store the k0rdent images. If you don't have a registry, you can deploy a local Docker registry or use mindthegap to provide one.</p> <p>WARNING: If using a local Docker registry, ensure the registry URL is added to the <code>insecure-registries</code> key within the Docker <code>/etc/docker/daemon.json</code> file, as in: <pre><code>{\n  \"insecure-registries\": [\"&lt;registry-url&gt;\"]\n}\n</code></pre></p> </li> <li> <p>A registry and associated chart repository for hosting kcm charts.  At this   time, all k0rdent charts MUST be hosted in a single OCI chart repository.  See   Use OCI-based registries in the   Helm documentation for more information.</p> </li> <li>jq, Helm and Docker binaries installed on the machine from which the <code>airgap-push.sh</code> script (see below) will run. [[[ THIS IS THE SAME MACHINE WHERE THE CLUSTER IS RUNNING, NO? ]]]</li> </ul>"},{"location":"admin-install/#installation_1","title":"Installation","text":"<ol> <li> <p>Download the kcm airgap bundle from [[[ INSERT URL HERE ]]]. This bundle contains:</p> <ul> <li><code>images/kcm-images-&lt;version&gt;.tgz</code> - The image bundle tarball for the   management cluster. This bundle is loaded into the management   cluster.</li> <li><code>images/kcm-extension-images-&lt;version&gt;.tgz</code> - The image bundle tarball for   the managed clusters. This bundle will be pushed to a registry where the   images can be accessed by managed clusters when deployed.</li> <li><code>charts</code> - Contains the kcm Helm chart, dependency charts and k0s   extensions charts within the <code>extensions</code> directory.  All of these charts   get pushed to a chart repository within a registry.</li> <li><code>scripts/airgap-push.sh</code> - A script that will aid in re-tagging and   pushing the <code>ClusterDeployment</code> required charts and images to a desired   registry.</li> </ul> </li> <li> <p>Log into the container and chart registries:</p> <pre><code>docker login\nhelm registry login\n</code></pre> </li> <li> <p>Extract and use the <code>airgap-push.sh</code> script to push the <code>extensions</code> images    and <code>charts</code> contents to the registry:  </p> <pre><code>tar xvf kcm-airgap-&lt;version&gt;.tgz scripts/airgap-push.sh\n./scripts/airgap-push.sh -r &lt;registry&gt; -c &lt;chart-repo&gt; -a kcm-airgap-&lt;version&gt;.tgz\n</code></pre> </li> <li> <p>Extract the <code>management</code> bundle tarball and sync the images to the    target k0s management cluster.  See Sync the Bundle File    for more information.</p> <p>NOTE: Multiple image bundles can be placed in the <code>/var/lib/k0s/images</code> directory for k0s to use; you don't need to merge the existing <code>k0s</code> airgap bundle into the <code>kcm-images-&lt;version&gt;.tgz</code> bundle.</p> <pre><code>tar -C /var/lib/k0s -xvf kcm-airgap-&lt;version&gt;.tgz \"images/kcm-images-&lt;version&gt;.tgz\"\n</code></pre> </li> <li> <p>At this point the chart registry includes the k0rdent kcm charts. The kcm controller image is loaded as part of the airgap <code>management</code> bundle and does not need to be customized within the Helm chart, but the default chart repository configured via <code>controller.defaultRegistryURL</code> should be set to reference the repository:</p> <pre><code>helm install kcm oci://&lt;chart-repository&gt;/kcm \\\n  --version &lt;version&gt; \\\n  -n kcm-system \\\n  --create-namespace \\\n  --set controller.defaultRegistryURL=oci://&lt;chart-repository&gt;\n</code></pre> </li> <li> <p>Edit the <code>Management</code> object to add the airgap parameters.</p> <p>The resulting yaml should look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi:\n      config:\n        airgap: true\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: oci://&lt;registry-url&gt;\n          insecureRegistry: true\n  providers:\n  - config:\n      airgap: true\n    name: k0smotron\n  - config:\n      airgap: true\n    name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: &lt;release name&gt;\n</code></pre> <p>NOTE: Use the <code>insecureRegistry</code> parameter only if you have a plain HTTP (rather than HTTPS) registry.</p> </li> <li> <p>k0s components must be downloaded at each node upon creation. To make them available, you can either place the k0s binary and airgap bundle on an internal server so they are available over HTTP, or make them available as a Kubernetes <code>Deployment</code> as follows:</p> <p>NOTE: k0s image version is the same that the default defined in the vSphere template. [[[ NOT SURE WHAT THIS SENTENCE MEANS ]]]</p> </li> </ol> <p><pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k0s-ag-image\n  labels:\n    app: k0s-ag-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k0s-ag-image\n  template:\n    metadata:\n      labels:\n        app: k0s-ag-image\n    spec:\n      containers:\n      - name: k0s-ag-image\n      image: k0s-ag-image:v1.31.1-k0s.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: k0s-ag-image\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: k0s-ag-image\n  type: NodePort\n</code></pre>     [[[ EXPLAIN WHAT IS GOING ON HERE ]]]</p>"},{"location":"admin-install/#creation-of-the-clusterdeployment","title":"Creation of the ClusterDeployment","text":"<p>The last step is to make sure that <code>ClusterDeployment</code>s have access to the Kubernetes bundle. To do that, you need to add additional information to the <code>.spec.config</code> of the <code>ClusterDeployment</code> object.</p> <p>Specifically, you need to specify the custom image registry and chart repository to be used. These are the same URLs to which you pushed the <code>extensions</code> bundle and charts earlier. Finally, provide the endpoint where the k0s binary and airgap bundle can be downloaded, as you specified in step 7 of the installation procedure):</p> <pre><code>spec:\n config:\n   airgap: true\n   k0s:\n     downloadURL: \"http://&lt;k0s binary endpoint&gt;/k0s\"\n     bundleURL: \"http://&lt;k0s binary endpoint&gt;/k0s-airgap-bundle\"\n   extensions:\n    imageRepository: ${IMAGE_REPOSITORY}\n    chartRepository: ${CHART_REPOSITORY}\n</code></pre>"},{"location":"admin-install/#managed-cluster-target-platforms","title":"Managed Cluster Target Platforms","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>Amazon Web Services</li> <li>Microsoft Azure</li> <li>OpenStack</li> <li>VMware</li> </ul>"},{"location":"admin-install/#aws","title":"AWS","text":"<p>k0rdent is able to deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in [[[ add link ]]] Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>k0rdent uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. clusterawsadm, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account k0rdent will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to crete the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables clusterawsadmin will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;aws-region&gt;\nexport AWS_ACCESS_KEY_ID=&lt;admin-user-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;admin-user-secret-access-key&gt;\nexport AWS_SESSION_TOKEN=&lt;session-token&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation </p> <p>Now use clusterawsadm to create the IAM CloudFormation stack:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the aws CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n  \"User\": {\n      \"Path\": \"/\",\n      \"UserName\": \"k0rdentQuickstart\",\n      \"UserId\": \"AIDA2XXXXXXXXXXXXXXXX\",\n      \"Arn\": \"arn:aws:iam::743175908171:user/k0rdentQuickstart\",\n      \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n      {\n          \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n          \"Arn\": \"arn:aws:iam::743175908171:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n          \"Arn\": \"arn:aws:iam::743175908171:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n      },\n      {\n          \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n          \"Arn\": \"arn:aws:iam::743175908171:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n          \"Arn\": \"arn:aws:iam::743175908171:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::743175908171:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::743175908171:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::743175908171:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentQuickstart \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n      \"UserName\": \"k0rdentQuickstart\",\n      \"AccessKeyId\": \"AKIA2XXXXXXXXXXXXXXX\",\n      \"Status\": \"Active\",\n      \"SecretAccessKey\": \"nWtQIsXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n      \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials Secret on the k0rdent Management Cluster</p> <p>Create a YAML file called aws-cluster-identity-secret.yaml and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-cluster-identity-secret\nnamespace: kcm-system\ntype: Opaque\nstringData:\nAccessKeyID: EXAMPLE_ACCESS_KEY_ID\nSecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently kcm-system) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named aws-cluster-identity.yaml:</p> <pre><code>kind: AWSClusterStaticIdentity\nmetadata:\nname: aws-cluster-identity\nspec:\nsecretRef: aws-cluster-identity-secret\nallowedNamespaces:\n  selector:\n    matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the kcm <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\nname: aws-cluster-identity-cred\nnamespace: kcm-system\nspec:\ndescription: \"Credential Example\"\nidentityRef:\n  apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n  kind: AWSClusterStaticIdentity\n  name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\nname: my-aws-clusterdeployment1\nnamespace: kcm-system\nspec:\ntemplate: aws-standalone-cp-0-0-5\ncredential: aws-cluster-identity-cred\nconfig:\n  region: us-east-2\n  controlPlane:\n    instanceType: t3.small\n  worker:\n    instanceType: t3.small\n</code></pre> <p>A couple of things to notice: - You're giving it an arbitrary name (my-aws-clusterdeployment1) - You're referencing the credential you created in the previous step. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand. - You need to choose a template to use for the cluster. You can get a list of available templates using: <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with:</p> <p><pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p> <p>When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>When you've established that it's working properly. you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 \n</code></pre> </li> </ol>"},{"location":"admin-install/#azure","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances.</p>"},{"location":"admin-install/#software-prerequisites","title":"Software prerequisites","text":"<p>Before using k0rdent to deploy managed Kubernetes clusters on Azure, make sure you have the following components installed and configured:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in [[[ add link ]]] Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (az) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> </li> <li> <p>Register resource providers</p> <p>In order for k0rdent to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <ul> <li><code>Microsoft.Compute</code></li> <li><code>Microsoft.Network</code></li> <li><code>Microsoft.ContainerService</code></li> <li><code>Microsoft.ManagedIdentity</code></li> <li><code>Microsoft.Authorization</code></li> </ul> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> </ol>"},{"location":"admin-install/#credentials","title":"Credentials","text":"<p>Creating a managed cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on k0rdent [[[ LINK TO CREDENTIALS FILE ]]]  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. To Set up credentials, follow these steps:</p> <ol> <li> <p>Find Your Subscription ID</p> <p>Your Azure must have at least one subscription for you to use it with k0rdent, so if you're working with a new account make sure to (create a new subscription with billing information)[] before you start.</p> <p>To get the information you need, list all your Azure subscriptions:https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/initial-subscriptions</p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    12345678-1234-5678-1234-567812345678  87654321-1234-5678-1234-12345678\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"12345678-7848-4ce6-9be9-a4b3eecca0ff\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"12~34~I5zKrL5Kem2aXsXUw6tIig0M~3~1234567\",\n\"tenant\": \"12345678-959b-481f-b094-eb043a87570a\"\n}\n</code></pre> Note that this information gives you access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a Secret object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called azure-cluster-identity-secret.yaml:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: &lt;password&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named azure-cluster-identity.yaml:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;appId&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;tenant&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as azure-cluster-identity-cred.yaml.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p>"},{"location":"admin-install/#create-your-first-clusterdeployment","title":"Create your first ClusterDeployment","text":"<p>To test the configuration, deploy a managed cluster by following these steps:</p> <ol> <li> <p>Determine the region</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n\u2026\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> </li> <li> <p>Create the cluster specification</p> <p>To create a managed cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;subscription-id&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure k0rdent isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> </ol>"},{"location":"admin-install/#cleanup","title":"Cleanup","text":"<p>To clean up Azure resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p>"},{"location":"admin-install/#openstack","title":"OpenStack","text":"<p>Much of the following includes the process of setting up credentials for OpenStack. To better understand how k0rdent uses credentials, read the Credential system.</p> <p>To configure and test k0rdent's ability to create OpenStack managed clusters, follow these steps:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>This credential should include:</p> <ul> <li><code>OS_AUTH_URL</code></li> <li><code>OS_APPLICATION_CREDENTIAL_ID</code></li> <li><code>OS_APPLICATION_CREDENTIAL_SECRET</code></li> <li><code>OS_REGION_NAME</code></li> <li><code>OS_INTERFACE</code></li> <li><code>OS_IDENTITY_API_VERSION</code> (commonly <code>3</code>)</li> <li><code>OS_AUTH_TYPE</code> (e.g., <code>v3applicationcredential</code>)</li> </ul> <p>While it's possible to use a username and password instead of the Application Credential--adjust your YAML accordingly--an  Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes Secret containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the k0rdent Credential Object</p> <p>Next, define a Credential that references the Secret from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the <code>Secret</code>\u2019s namespace (<code>kcm-system</code>).</p> </li> <li> <p>Create Your First Managed Cluster</p> <p>To test the configuration, create YAML with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>Here is an example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-1\n  credential: openstack-cluster-identity-cred\n  config:\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: &lt;OS_AUTH_URL&gt;\n</code></pre> You can adjust <code>flavor</code>, <code>image</code> name, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value&gt;' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-install/#vsphere","title":"VSphere","text":"<p>Much of the following includes the process of setting up credentials for vSphere. To better understand how k0rdent uses credentials, read the Credential System.</p>"},{"location":"admin-install/#prerequisites_2","title":"Prerequisites","text":"<p>Make sure you have the following installed:</p> <ol> <li> <p>A k0rdent management cluster</p> <p>Follow the instructions in install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The <code>kubectl</code> CLI, installed locally.</p> </li> <li> <p>A vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Image template.</p> </li> <li> <p>vSphere network with DHCP enabled.</p> </li> <li> <p>vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <ul> <li><code>Virtual machine</code>: Full permissions are required</li> <li><code>Network</code>: <code>Assign network</code> is sufficient</li> <li><code>Datastore</code>: The user should be able to manipulate virtual machine files and metadata</li> </ul> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> </ol>"},{"location":"admin-install/#create-credentials","title":"Create credentials","text":"<p>To enable k0rdent to access vSphere resources, create the appropriate credentials objects:</p> <ol> <li> <p>Create a <code>Secret</code> Object with the username and password</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  username: &lt;user&gt;\n  password: &lt;password&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSpehereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> </ol>"},{"location":"admin-install/#create-your-first-cluster-deployment","title":"Create your first Cluster Deployment","text":"<p>Test the configuration by deploying a cluster. Create a YAML document with the specification of  your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <pre><code>kubectl get clustertemplate -A\n</code></pre> <p>Here is an example of a <code>ClusterDeployment</code> YAML file. Make sure the replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-0-0-5\n  credential: vsphere-cluster-identity-cred\n  config:\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> <p>To delete provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre>"},{"location":"admin-install/#finding-releases","title":"Finding Releases","text":"<p>Releases are tagged in the GitHub repository and can be found here.</p>"},{"location":"admin-install/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>k0rdent is deployed with the following default configuration, which may vary depending on the release version:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre> To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>There are two options to override the default management configuration of k0rdent:</p> <ol> <li> <p>Update the <code>Management</code> object after the k0rdent installation using <code>kubectl</code>:</p> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> </li> <li> <p>Deploy k0rdent skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li>Create <code>management.yaml</code> file and configure core components and providers.</li> <li> <p>Specify <code>--create-management=false</code> controller argument and install k0rdent:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> <p><code>--set=\"controller.createManagement=false\"</code></p> </li> <li> <p>Create the <code>kcm</code> <code>Management</code> object after the k0rdent installation:</p> </li> </ul> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ol>"},{"location":"admin-install/#cleanup_1","title":"Cleanup","text":"<p>Before you delete the <code>Management</code> object, make sure that you've deleted all <code>ClusterDeployment</code>s, as you won't be able to delete them afterwards.</p> <ol> <li>Remove the Management object:</li> </ol> <pre><code>  kubectl delete management kcm\n</code></pre> <ol> <li>Remove the <code>kcm</code> Helm release:</li> </ol> <pre><code>  helm uninstall kcm -n kcm-system\n</code></pre> <ol> <li>Remove the <code>kcm-system</code> namespace:</li> </ol> <pre><code>  kubectl delete ns kcm-system\n</code></pre>"},{"location":"guide_to_k0rdent_documentation/","title":"Guide to k0rdent documentation","text":"<p>k0rdent\u2019s documentation has five main parts:</p> <ul> <li> <p>Welcome to k0rdent: Go here to learn what k0rdent is all about, why we're building' it, and the basics of how it can be used, day to day.</p> <ul> <li> <p>Why k0rdent?: What k0rdent is, how it\u2019s different, and why you might use it.</p> </li> <li> <p>K0rdent Architecture: Just the basics: understanding k0rdent\u2019s components and how they work together.</p> </li> </ul> </li> <li> <p>QuickStarts and Tutorials: Go here if you\u2019re in a hurry.</p> <ul> <li>QuickStart 1 - k0rdent pre-requisites Installation basics</li> <li>QuickStart 2 - AWS infrastructure setup Setting up an AWS test environment for k0rdent</li> <li>QuickStart 3 - Deploy managed clusters on AWS Deploying with ClusterTemplates and ManagedCluster objects</li> <li>Tutorial 1 - Upgrade a Single Standalone Cluster</li> <li>Tutorial 2 - Install a ServiceTemplate into a single standalone cluster</li> <li>Tutorial 3 - Install a ServiceTemplate into multiple clusters</li> <li>Tutorial 4 - Approve a ClusterTemplate &amp; InfraCredentials for a separate namespace</li> <li>Tutorial 5 - Approve a ServiceTemplate for a separate namespace</li> <li>Tutorial 6 - Use an approved ClusterTemplate in a separate namespace</li> <li>Tutorial 7 - Use an approved ServiceTemplate in a separate namespace</li> </ul> </li> <li> <p>Reference Guides: Deeper references for using k0rdent with teams and in production.</p> <ul> <li> <p>Administrator Guide: Using k0rdent in production. This section is for CloudOps administrators who will be installing k0rdent and making it available to Project Teams who will use it to deploy applications.</p> </li> <li> <p>User Guide: Once k0rdent is in place, this guide helps Project Teams, developers, and other users leverage k0rdent securely and effectively.</p> </li> <li> <p>Template Reference Guide: k0rdent works on the basis of templates, describing platforms and infrastructures. This guide explains how k0rdent templates work and how to modify and create them.</p> </li> </ul> </li> <li> <p>Community: Who works on k0rdent, how to get involved, and how to contribute.</p> </li> </ul>"},{"location":"guide_to_quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"guide_to_quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"guide_to_quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"guide_to_quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or cloud virtual machine running a supported version of Ubuntu Server (e.g., 22.04.5 LTS, Jammy Jellyfish) \u2014 This machine will be used to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows:<ul> <li>A minimum of 32GB RAM, 8 vCPUs, 100GB SSD (e.g., AWS <code>t3.2xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (i.e., edit /etc/sudoers to configure your user to issue sudo commands without a password challenge)</li> <li>Inbound traffic - SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic - All to any IP address</li> <li>Apply all recent updates and upgrade local applications (sudo apt update/sudo apt upgrade)</li> <li>(Optional) snapshot the machine in its virgin state</li> </ul> </li> <li>Administrative-level access to an AWS or Azure cloud account - Depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting managed clusters.</li> </ul> <p>Note: Ubuntu is a Debian distro and uses <code>apt</code> for package management. Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (e.g., Docker). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"guide_to_quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal k0rdent working environment. Setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Amazon cloud environments, and guides in creating 'standalone' clusters \u2014 in k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' is the k0s Kubernetes distro.</p> <p>However: this is far from being all that k0rdent can do today. So ...</p>"},{"location":"guide_to_quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Ready? Let's discover how to use k0rdent!</p>"},{"location":"k0rdent_architecture/","title":"k0rdent architecture","text":"<p>Text coming. Draft in GDocs. </p>"},{"location":"quickstart_1_management_node_and_cluster/","title":"QuckStart 1 - Set up Management Node and Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting k0rdent components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive k0rdent by SSHing into the management node (kubectl is there and will be provisioned with the appropriate kubeconfig) or remotely by various means (e.g., install the management cluster kubeconfig in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local kubectl, etc.)</p>"},{"location":"quickstart_1_management_node_and_cluster/#install-a-single-node-k0s-cluster-locally-to-work-as-k0rdents-management-cluster","title":"Install a single-node k0s cluster locally to work as k0rdent's management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command, and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, Team k0rdent would love to hear how you set things up to work for you.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging kubectl (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible kubectl and makes it accessible via its own client. But to make your environment easier to configure, we advise installing kubectl the normal way on the manager node and using it to control the local k0s management cluster.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's kubeconfig in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed kubectl should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-k0rdent-into-the-k0s-management-cluster","title":"Install k0rdent into the k0s management cluster","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/mirantis/hmc/charts/hmc:0.0.3\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: hmc\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: hmc-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstart_1_management_node_and_cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures: i.e., it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstart_1_management_node_and_cluster/#check-that-the-sveltos-pods-are-running","title":"Check that the sveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent Service Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the sveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait a little while for all the pods to reconcile and start running.</p>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantied in the management cluster as objects. Before continuing, verify that default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace \u2014 for example, the templates that determine setup of KCM itself and other parts of the k0rdent system (e.g., projectsveltos, which is a component of k0rdent Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (i.e., control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below. The placeholder X-Y-Z represents the current version number of the template, and will be replaced in the listing with digits:</p> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantied in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default ClusterTemplate objects are installed and verified:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>k0rdent Service Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as ServiceTemplate objects. Check that default ServiceTemplate objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstart_2_AWS/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS), and deploying our first managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage AWS \u2014 doing so via the CAPA (ClusterAPI for AWS) provider. So a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new 'k0rdent user' on your account with the particular permissions k0rdent and CAPA require.</p> <p>In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to k0rdent in the management node. Note: if you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart_2_AWS/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstart_2_AWS/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages clusterawsadm, a CLI tool created by CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install clusterawsadm on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstart_2_AWS/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by clusterawsadm (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstart_2_AWS/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <p>You'll see something like what's shown below. You should save this data securely. Note the Amazon Resource Name (ARN) because we'll be using it right away:</p> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user. In the above example of content returned from the AWS CLI on user creation (see above) that's marked with the placeholder <code>FAKE_ARN_123</code>:</p> <p>Given this, you can assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were assigned:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"Policies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to prepare for this by using <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstart_2_AWS/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a secret containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the kcm-system namespace (important: if you use another namespace, k0rdent will be unable to read the credentials). To do this, create the following YAML in a file called `aws-cluster-identity-secret.yaml':</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, k0rdentQuickStart.</p> <p>Then apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in immediately prior steps, create a YAML file called `aws-cluster-identity-cred.yaml':</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that .spec.identityRef.kind must be AWSClusterStaticIdentity and .spec.identityRef.name must match the .metadata.name of the AWSClusterStaticIdentity object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart_2_AWS/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"quickstart_2_AWS/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output like what's below:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_AWS/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart_2_AWS/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_AWS/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart_2_AWS/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"quickstart_2_Azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our AWS QuickStart (QuickStart 2 - AWS target environment) you can continue here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage Azure resources \u2014 doing so via the CAPZ (ClusterAPI for Azure) provider. So a best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new k0rdent Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require.</p> <p>In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node. Note: if you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart_2_Azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstart_2_Azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstart_2_Azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstart_2_Azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstart_2_Azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing  with the ID you copied earlier. <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstart_2_Azure/#create-a-secret-object-with-the-sp-password","title":"Create a Secret object with the SP password","text":"<p>Now we'll create a new Secret to store the Service Principal password.</p> <p>Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quickstart_2_Azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p>```shell: kubectl apply -f azure-cluster-identity.yaml <pre><code>You should see output resembling this:\n\n```console\nazureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p>"},{"location":"quickstart_2_Azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that <code>.spec.kind</code> must be <code>AzureClusterIdentity</code> and <code>.spec.name</code> must match <code>.metadata.name</code> of the AzureClusterIdentity object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstart_2_Azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (center column) of the region you wish to deploy to.</p>"},{"location":"quickstart_2_Azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>azure-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart_2_Azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstart_2_Azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstart_2_Azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart_2_Azure/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_Azure/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart_2_Azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"why_k0rdent/","title":"Why k0rdent?","text":"<p>With the increasing complexity of modern systems, it has become more and more necessary to provide a way for administrators to manage that complexity while still providing the means for developers to get their jobs done as efficiently as possible.  Enter platforms. These frameworks or environments provide the foundation and tools for developing, deploying, and managing applications, services, or systems, enabling users to focus on their specific tasks or goals while abstracting away the underlying complexities.</p> <p>As the need for platform engineering, the practice of designing, building, and maintaining these platforms, becomes evident, so do some of the challenges of creating a truly functional platform engineering environment. Assembling, integrating, deploying, and managing Kubernetes clusters is straightforward in theory, but where companies may have once had a single multi-purpose platform, the current trend is to implement many more purpose-dedicated, simpler platforms. While this makes managing each platform easier, you\u2019re left with a sum that\u2019s more than its parts, both in capability and complexity. Once you add in multiple clouds, differing infrastructures, and the need to prepare those infrastructures to host workloads, platform engineering moves from being a task on an administrator\u2019s plate to being an entire discipline.</p> <p>The solution must be answerable to real world needs. This moves beyond simple resilience and availability to awareness of security and compliance requirements, including access management and sharing. Platform engineers need to be able to compose platforms\u2013for example, Kubernetes, beach-head services such as CI/CD, and any other dependencies workloads require\u2013in simple, repeatable ways. They also need to be able to safely and flexibly share access and artifacts with platform leads and other platform 'consumers/users.' On top of that, platforms must enable visibility into both the infrastructure and applications, including both performance and cost monitoring and ensuring platform integrity with state monitoring and continuous reconciliation, all while simplifying platform operations for Day 2 and beyond.</p> <p>All of these functionalities are available in open source. Kubernetes is the most obvious substrate, of course, providing tools for not only container orchestration, but also custom operators. Fortunately, there are optimized distros of Kubernetes such as k0s, and Kubernetes itself can be orchestrated using the Kubernetes Cluster API (CAPI), which can both manage clouds and infrastructure and assemble, create and lifecycle manage clusters on those infrastructures. Add open source tools such as Helm and other package managers to help manage deployment, upgrade, and removal of applications, and you have a complete platform engineering solution.</p> <p>Of course, getting all of the relevant pieces to work together\u2013particularly over multiple infrastructures\u2013isn\u2019t simple or straightforward. That\u2019s why you need k0rdent. k0rdent puts all of these pieces together so you don\u2019t have to.</p>"}]}
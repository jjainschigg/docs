{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent is focused on developing a consistent way to deploy and manage Kubernetes clusters at scale. One way to think of k0rdent is as a \"super control plane\" designed to manage other Kubernetes control planes. Another way to think of k0rdent is as a platform for Platform Engineering. If you are building an internal developer platform (IDP), need a way to manage Kubernetes clusters at scale in a centralized place, create Golden Paths, etc. k0rdent is a great way to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do so. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a-vs-hmc-naming","title":"k0rdent vs Project 2A vs HMC naming","text":"<p>k0rdent is the official name of an internal Mirantis project that was originally codenamed \"Project 2A\". During our initial skunkworks-style 3-month MVP push, the code was put into a repository named HMC, which stood for \"Hybrid Multi-Cluster Controller\". What is HMC became k0rdent Cluster Manager (kcm), but it may be a little confusing because the overall project was still called \"Project 2A\" or even \"HMC\" at times.</p> <p>So, to be clear, here are the names and components:</p> <ul> <li>k0rdent: the overall project name</li> <li>k0rdent Cluster Manager (kcm)</li> <li>k0rdent State Manager (ksm)<ul> <li>This is currently rolled into kcm, but will be split out in the   future</li> <li>ksm leverages Project Sveltos   for certain functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (kof)</li> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and kcm   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (kcm)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (ksm)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations and more.</p> </li> <li> <p>k0rdent Observability and FinOps (kof)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to development process and developer specific notes located in the main repository.</p>"},{"location":"guide_to_k0rdent_documentation/","title":"Guide to k0rdent documentation","text":"<p>k0rdent\u2019s documentation has five main parts:</p> <ul> <li> <p>Welcome to k0rdent: Go here to learn what k0rdent is all about, why we're building' it, and the basics of how it can be used, day to day.</p> <ul> <li> <p>Why k0rdent?: What k0rdent is, how it\u2019s different, and why you might use it.</p> </li> <li> <p>K0rdent Architecture: Just the basics: understanding k0rdent\u2019s components and how they work together.</p> </li> </ul> </li> <li> <p>QuickStarts and Tutorials: Go here if you\u2019re in a hurry.</p> <ul> <li>QuickStart 1 - k0rdent pre-requisites Installation basics</li> <li>QuickStart 2 - AWS infrastructure setup Setting up an AWS test environment for k0rdent</li> <li>QuickStart 3 - Deploy managed clusters on AWS Deploying with ClusterTemplates and ManagedCluster objects</li> <li>Tutorial 1 - Upgrade a Single Standalone Cluster</li> <li>Tutorial 2 - Install a ServiceTemplate into a single standalone cluster</li> <li>Tutorial 3 - Install a ServiceTemplate into multiple clusters</li> <li>Tutorial 4 - Approve a ClusterTemplate &amp; InfraCredentials for a separate namespace</li> <li>Tutorial 5 - Approve a ServiceTemplate for a separate namespace</li> <li>Tutorial 6 - Use an approved ClusterTemplate in a separate namespace</li> <li>Tutorial 7 - Use an approved ServiceTemplate in a separate namespace</li> </ul> </li> <li> <p>Reference Guides: Deeper references for using k0rdent with teams and in production.</p> <ul> <li> <p>Administrator Guide: Using k0rdent in production. This section is for CloudOps administrators who will be installing k0rdent and making it available to Project Teams who will use it to deploy applications.</p> </li> <li> <p>User Guide: Once k0rdent is in place, this guide helps Project Teams, developers, and other users leverage k0rdent securely and effectively.</p> </li> <li> <p>Template Reference Guide: k0rdent works on the basis of templates, describing platforms and infrastructures. This guide explains how k0rdent templates work and how to modify and create them.</p> </li> </ul> </li> <li> <p>Community: Who works on k0rdent, how to get involved, and how to contribute.</p> </li> </ul>"},{"location":"guide_to_quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Emphasis is on quick and simple, not production-ready. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"guide_to_quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"guide_to_quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"guide_to_quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or cloud virtual machine running a supported version of Ubuntu Server (e.g., 22.04.5 LTS, Jammy Jellyfish) \u2014 This machine will be used to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows: ** A minimum of 32GB RAM, 8 vCPUs, 100GB SSD (e.g., AWS <code>t3.2xlarge</code> or equivalent) ** Set up for SSH access using keys (standard for cloud VMs) ** Set up for passwordless sudo (i.e., edit /etc/sudoers to configure your user to issue sudo commands without a password challenge) ** Inbound traffic - SSH (port 22) and ping from your laptop's IP address ** Outbound traffic - All to any IP address ** Apply all recent updates and upgrade local applications (sudo apt update/sudo apt upgrade) ** (Optional) snapshot the machine in its virgin state</li> <li>Administrative-level access to an AWS or Azure cloud account - Depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting managed clusters.</li> </ul> <p>Note: Ubuntu is a Debian distro and uses <code>apt</code> for package management. Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (e.g., Docker). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"guide_to_quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal, non-production k0rdent working environment. It is meant to be ephemeral, and is set up more for simplicity and convenience than for scalability and security. As noted above, setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Amazon cloud environments, and guides in creating 'standalone' clusters \u2014 in k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' is the k0s Kubernetes distro.</p> <p>However: this is far from being all that k0rdent can do today. So ...</p>"},{"location":"guide_to_quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Ready? Let's discover how to use k0rdent!</p>"},{"location":"k0rdent_architecture/","title":"k0rdent architecture","text":"<p>Text coming. Draft in GDocs. </p>"},{"location":"quickstart_1_management_node_and_cluster/","title":"QuckStart 1 - Set up Management Node and Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting k0rdent components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive k0rdent by SSHing into the management node (kubectl is there and will be provisioned with the appropriate kubeconfig) or remotely by various means (e.g., install the management cluster kubeconfig in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local kubectl, etc.)</p>"},{"location":"quickstart_1_management_node_and_cluster/#install-a-single-node-k0s-cluster-locally-to-work-as-k0rdents-management-cluster","title":"Install a single-node k0s cluster locally to work as k0rdent's management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command, and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, Team k0rdent would love to hear how you set things up to work for you.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging kubectl (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible kubectl and makes it accessible via its own client. But to make your environment easier to configure, we advise installing kubectl the normal way on the manager node and using it to control the local k0s management cluster.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's kubeconfig in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed kubectl should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#install-k0rdent-into-the-k0s-management-cluster","title":"Install k0rdent into the k0s management cluster","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/mirantis/hmc/charts/hmc:0.0.3\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: hmc\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: hmc-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstart_1_management_node_and_cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures: i.e., it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstart_1_management_node_and_cluster/#check-that-the-sveltos-pods-are-running","title":"Check that the sveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent Service Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the sveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait a little while for all the pods to reconcile and start running.</p>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantied in the management cluster as objects. Before continuing, verify that default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace \u2014 for example, the templates that determine setup of KCM itself and other parts of the k0rdent system (e.g., projectsveltos, which is a component of k0rdent Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (i.e., control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below. The placeholder X-Y-Z represents the current version number of the template, and will be replaced in the listing with digits:</p> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantied in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default ClusterTemplate objects are installed and verified:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>k0rdent Service Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as ServiceTemplate objects. Check that default ServiceTemplate objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstart_1_management_node_and_cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstart_2_AWS/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS), and deploying our first managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage AWS \u2014 doing so via the CAPA (ClusterAPI for AWS) provider. So a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new 'k0rdent user' on your account with the particular permissions k0rdent and CAPA require.</p> <p>In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to k0rdent in the management node. Note: if you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart_2_AWS/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstart_2_AWS/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages clusterawsadm, a CLI tool created by CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install clusterawsadm on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstart_2_AWS/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by clusterawsadm (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstart_2_AWS/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <p>You'll see something like what's shown below. You should save this data securely. Note the Amazon Resource Name (ARN) because we'll be using it right away:</p> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user. In the above example of content returned from the AWS CLI on user creation (see above) that's marked with the placeholder <code>FAKE_ARN_123</code>:</p> <p>Given this, you can assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were assigned:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"Policies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart_2_AWS/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to prepare for this by using <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstart_2_AWS/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a secret containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the kcm-system namespace (important: if you use another namespace, k0rdent will be unable to read the credentials). To do this, create the following YAML in a file called `aws-cluster-identity-secret.yaml':</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, k0rdentQuickStart.</p> <p>Then apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in immediately prior steps, create a YAML file called `aws-cluster-identity-cred.yaml':</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that .spec.identityRef.kind must be AWSClusterStaticIdentity and .spec.identityRef.name must match the .metadata.name of the AWSClusterStaticIdentity object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart_2_AWS/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart_2_AWS/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"quickstart_2_AWS/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output like what's below:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_AWS/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart_2_AWS/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_AWS/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart_2_AWS/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"quickstart_2_Azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our AWS QuickStart (QuickStart 2 - AWS target environment) you can continue here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage Azure resources \u2014 doing so via the CAPZ (ClusterAPI for Azure) provider. So a best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new k0rdent Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require.</p> <p>In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node. Note: if you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart_2_Azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstart_2_Azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstart_2_Azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstart_2_Azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstart_2_Azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing  with the ID you copied earlier. <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstart_2_Azure/#create-a-secret-object-with-the-sp-password","title":"Create a Secret object with the SP password","text":"<p>Now we'll create a new Secret to store the Service Principal password.</p> <p>Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quickstart_2_Azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p>```shell: kubectl apply -f azure-cluster-identity.yaml <pre><code>You should see output resembling this:\n\n```console\nazureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p>"},{"location":"quickstart_2_Azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that <code>.spec.kind</code> must be <code>AzureClusterIdentity</code> and <code>.spec.name</code> must match <code>.metadata.name</code> of the AzureClusterIdentity object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstart_2_Azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (center column) of the region you wish to deploy to.</p>"},{"location":"quickstart_2_Azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>azure-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart_2_Azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstart_2_Azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstart_2_Azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart_2_Azure/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart_2_Azure/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart_2_Azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"why_k0rdent/","title":"Why k0rdent?","text":"<p>With the increasing complexity of modern systems, it has become more and more necessary to provide a way for administrators to manage that complexity while still providing the means for developers to get their jobs done as efficiently as possible.  Enter platforms. These frameworks or environments provide the foundation and tools for developing, deploying, and managing applications, services, or systems, enabling users to focus on their specific tasks or goals while abstracting away the underlying complexities.</p> <p>As the need for platform engineering, the practice of designing, building, and maintaining these platforms, becomes evident, so do some of the challenges of creating a truly functional platform engineering environment. Assembling, integrating, deploying, and managing Kubernetes clusters is straightforward in theory, but where companies may have once had a single multi-purpose platform, the current trend is to implement many more purpose-dedicated, simpler platforms. While this makes managing each platform easier, you\u2019re left with a sum that\u2019s more than its parts, both in capability and complexity. Once you add in multiple clouds, differing infrastructures, and the need to prepare those infrastructures to host workloads, platform engineering moves from being a task on an administrator\u2019s plate to being an entire discipline.</p> <p>The solution must be answerable to real world needs. This moves beyond simple resilience and availability to awareness of security and compliance requirements, including access management and sharing. Platform engineers need to be able to compose platforms\u2013for example, Kubernetes, beach-head services such as CI/CD, and any other dependencies workloads require\u2013in simple, repeatable ways. They also need to be able to safely and flexibly share access and artifacts with platform leads and other platform 'consumers/users.' On top of that, platforms must enable visibility into both the infrastructure and applications, including both performance and cost monitoring and ensuring platform integrity with state monitoring and continuous reconciliation, all while simplifying platform operations for Day 2 and beyond.</p> <p>All of these functionalities are available in open source. Kubernetes is the most obvious substrate, of course, providing tools for not only container orchestration, but also custom operators. Fortunately, there are optimized distros of Kubernetes such as k0s, and Kubernetes itself can be orchestrated using the Kubernetes Cluster API (CAPI), which can both manage clouds and infrastructure and assemble, create and lifecycle manage clusters on those infrastructures. Add open source tools such as Helm and other package managers to help manage deployment, upgrade, and removal of applications, and you have a complete platform engineering solution.</p> <p>Of course, getting all of the relevant pieces to work together\u2013particularly over multiple infrastructures\u2013isn\u2019t simple or straightforward. That\u2019s why you need k0rdent. k0rdent puts all of these pieces together so you don\u2019t have to.</p>"}]}
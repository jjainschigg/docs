{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k0rdent docs","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent is focused on developing a consistent way to deploy and manage Kubernetes clusters at scale. One way to think of k0rdent is as a \"super control plane\" designed to manage other Kubernetes control planes. Another way to think of k0rdent is as a platform for Platform Engineering. If you are building an internal developer platform (IDP), need a way to manage Kubernetes clusters at scale in a centralized place, create Golden Paths, etc. k0rdent is a great way to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do so. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a-vs-hmc-naming","title":"k0rdent vs Project 2A vs HMC naming","text":"<p>k0rdent is the official name of an internal Mirantis project that was originally codenamed \"Project 2A\". During our initial skunkworks-style 3-month MVP push, the code was put into a repository named HMC, which stood for \"Hybrid Multi-Cluster Controller\". What is HMC became k0rdent Cluster Manager (kcm), but it may be a little confusing because the overall project was still called \"Project 2A\" or even \"HMC\" at times.</p> <p>So, to be clear, here are the names and components:</p> <ul> <li>k0rdent: the overall project name</li> <li>k0rdent Cluster Manager (kcm)</li> <li>k0rdent State Manager (ksm)<ul> <li>This is currently rolled into kcm, but will be split out in the   future</li> <li>ksm leverages Project Sveltos   for certain functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (kof)</li> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and kcm   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (kcm)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (ksm)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations and more.</p> </li> <li> <p>k0rdent Observability and FinOps (kof)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to development process and developer specific notes located in the main repository.</p>"},{"location":"admin-adopting-clusters/","title":"Adopting an Existing Cluster","text":"<p>Creating a new cluster isn't the only way to use k0rdent. Adopting an existing Kubernetes cluster enables you to  bring it under the k0rdent's management. This process is useful when you already have a running cluster but want  to centralize management and leverage k0rdent's capabilities, such as unified monitoring, configuration, and automation.</p> <p>To adopt a cluster, k0rdent establishes communication between the management cluster (where kcm is installed)  and the target cluster. This requires proper credentials, network connectivity, and a standardized configuration. </p> <p>Follow these steps to adopt an existing cluster:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A kubeconfig file for the cluster you want to adopt (this file provides access credentials and configuration details    for the cluster).</li> <li>A management cluster with k0rdent installed and running. See the installation instructions    if you need to set it up.</li> <li>Network connectivity between the management cluster and the cluster to be adopted (for example, ensure firewall    rules and VPNs allow communication).</li> </ul> </li> <li> <p>Create a Credential</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure  provider. Follow the instructions in the Credential System, as well as the specific instructions  for your target infrastructure.</p> <p>Tip: Double-check that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Configure the Adopted Cluster Template</p> <p>Set the <code>KUBECONFIG</code> environment variable to the path of your management cluster's kubeconfig file so you can  execute commands against the management cluster.</p> <p>For example:</p> <pre><code>export KUBECONFIG=/path/to/management-cluster-kubeconfig\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code> YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is used to define how k0rdent should manage the adopted cluster. Create a  YAML file for the <code>ClusterDeployment</code> object, as shown below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: adopted-cluster-&lt;template-version&gt;\n  credential: &lt;credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>Replace placeholders like <code>&lt;cluster-name&gt;</code> and <code>&lt;credential-name&gt;</code> with actual values. The <code>dryRun</code> flag is useful for testing the configuration without making changes to the cluster. For more details, see the Dry Run section.</p> <p>You can also get a list of the available templates with:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n\nPutting it all together, your YAML would look something like this:\n\n```yaml\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adopted-cluster-0-0-2\n  credential: my-cluster-credential\n  dryRun: false\n</code></pre></p> </li> <li> <p>Apply the <code>ClusterDeployment</code> configuration</p> <p>Once your configuration file is ready, apply it to the management cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits the <code>ClusterDeployment</code> object to k0rdent, initiating the adoption process.</p> </li> <li> <p>Check the Status of the <code>ClusterDeployment</code> Object</p> <p>To ensure the adoption process is progressing as expected, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output includes the current state and any conditions (for example, errors or progress updates). Review  this information to confirm that the adoption is successful.</p> </li> </ol>"},{"location":"admin-adopting-clusters/#whats-happening-behind-the-scenes","title":"What\u2019s Happening Behind the Scenes?","text":"<p>When you adopt a cluster, k0rdent performs several actions: 1. It validates the credentials and configuration provided in the <code>ClusterDeployment</code> object. 2. It ensures network connectivity between the management cluster and the adopted cluster. 3. It registers the adopted cluster within the k0rdent system, enabling it to be monitored and managed like     any k0rdent-deployed cluster.</p> <p>This process doesn\u2019t change the adopted cluster\u2019s existing workloads or configurations. Instead, it enhances your  ability to manage the cluster through k0rdent.</p>"},{"location":"admin-adopting-clusters/#additional-tips","title":"Additional Tips","text":"<ul> <li>If you encounter issues, double-check that kubeconfig file you used for the adopted cluster is valid    and matches the cluster you\u2019re trying to adopt.</li> <li>Use the <code>dryRun</code> option during the first attempt to validate the configuration without making actual changes.</li> </ul>"},{"location":"admin-backup/","title":"Backing up k0rdent","text":""},{"location":"admin-before/","title":"Before you start","text":"<p>Before you start working with k0rdent, it helps to understand a few basics.</p>"},{"location":"admin-before/#how-k0rdent-works","title":"How K0rdent works","text":"<p>k0rdent has several important subsystems, notably:</p> <ul> <li>KCM - k0rdent Cluster Manager - KCM wraps and manages Kubernetes Cluster API, and lets you treat clusters as  Kubernetes objects. Within a k0rdent management cluster, you'll have a <code>ClusterDeployment</code> object that  represents a deployed cluster, with <code>Machine</code> objects, and so on. When you create a <code>ClusterDeployment</code>,  k0rdent deploys the cluster. When you delete it, k0rdent deletes it, and so on.</li> <li>KSM - k0rdent Service Manager - KSM wraps and manages several interoperating open source projects like Helm and Sveltos, which let you treat services and applications as Kubernetes objects.</li> </ul> <p>Together, KCM and KSM interoperate to manifest a complete, template-driven system for defining and managing complete Internal Development Platforms (IDPs) comprising suites of services, plus a cluster and its components as realized on a particular cloud or infrastructure substrate. </p> <p>[[[ DIAGRAM 1, SHOWING K0RDENT AND A CLUSTERDEPLOYMENT ]]]</p> <ul> <li>ClusterAPI providers: ClusterAPI uses <code>providers</code> to manage different clouds and infrastructures, including bare metal. k0rdent ships with providers for AWS, Azure, OpenStack and vSphere, and you can add additional providers in order to control other clouds or infrastructures that ClusterAPI supports.</li> </ul> <p>[[[ DIAGRAM 2, WHICH TAKES DIAGRAM 1 AND ADDS IN THE DRIVERS ]]]</p> <ul> <li>Templates: When you create a cluster, that cluster is based on a template, which specifies all of the various information about the cluster, such as where to find images, and so on. These templates get installed into k0rdent, but they don't do  anything until you reference them in a <code>ClusterDeployment</code> that represents an actual cluster.</li> </ul> <p>[[[ DIAGRAM 3, WHICH TAKES DIAGRAM 2 AND ADDS IN TEMPLATES AND CLUSTERDEPLOYMENTS ]]]</p> <p>k0rdent can also manage these clusters, upgrading, scaling them, or installing software and services.</p> <ul> <li>Services: To add (or manage) services, you also use templates. These <code>ServiceTemplate</code>s are like <code>ClusterTemplate</code>s, in that you install them into the cluster, but until they're actually referenced, they don't do anything. When you reference a <code>ServiceTemplate</code> as part of a <code>ClusterDeployment</code>, k0rdent knows to install that service into that cluster.</li> </ul> <p>[[[ DIAGRAM 4, WHICH TAKES DIAGRAM 3 AND ADDS SERVICETEMPLATES AND SERVICES ]]]</p> <p>These services can be actual services, such as Nginx or kyverno, or they can be user applications.</p>"},{"location":"admin-before/#how-credentials-work","title":"How Credentials work","text":"<p>Of course you can't do any of this without permissions. As a human, you can log into, say, AWS, and tell it to create a new instance on which you are going to install Kubernetes, but how does k0rdent get that permission? It gets it through the use of  <code>Credential</code>s. </p> <p>When you create a <code>ClusterDeployment</code> or deploy an application, you include a reference to a <code>Credential</code> object that has been installed in the k0rdent management cluster. Depending on whether the target infrastructure is AWS, Azure, or something else, that <code>Credential</code> might reference an access key and secret, or it might reference a service provider, but all of that gets abstracted out by the time you get to the <code>Credential</code>, which is what you'll actually reference.</p> <p>[[[ DIAGRAM 5, WHICH SHOWS THE DIFFERENT STRUCTURES THAT LEAD UP TO THE CREDENTIAL ]]]</p> <p>By abstracting everything out to create a standard <code>Credential</code> object, users never have to have access to actual credentials (lowercase \"c\"). This enables the administrator to keep those credentials private, and to rotate them as necessary without disturbing users or their applications. The administrator simply updates the <code>Credential</code> object and everything continues to work.</p> <p>You can find more information on creating these <code>Credential</code>s in the Credentials chapter.</p>"},{"location":"admin-before/#k0rdent-vs-gitops","title":"k0rdent vs GitOps","text":"<p>At its heart, k0rdent is a way to declaratively specify what should be happening in the infrastructure and have that maintained. In other words, if you want to, say, scale up a cluster, you would give that cluster a new definition that includes the additional nodes, and then k0rdent, seeing that reality doesn't match that definition,  will make it happen.</p> <p>In some ways that is very similar to GitOps, in which you commit definitions and tools such as Flux or ArgoCD  ensure that reality matches the definition. We can say that k0rdent is GitOps-compatible, in the sense that you can (and should) consider storing k0rdent templates and YAML object definitions in Git repos, and can (and may want to) use GitOps tools like ArgoCD to modify and manage them upstream of k0rdent itself.</p> <p>The main difference is that k0rdent's way of representing clusters and services is fully compliant with Kubernetes-native tools like ClusterAPI, Sveltos and Helm. So you can, in fact, port much of what you do with k0rdent templates and objects directly to other solution environments that leverage these standard tools.</p>"},{"location":"admin-creating-clusters/","title":"Creating and lifecycle-managing managed clusters","text":"<p>Once you've installed k0rdent, you can use it to create, manage, update and even upgrade clusters.</p>"},{"location":"admin-creating-clusters/#deploying-a-cluster","title":"Deploying a Cluster","text":"<p>k0rdent is designed to simplify the process of deploying and managing Kubernetes clusters across various cloud platforms. It does this through the use of <code>ClusterDeployment</code>s, which include all of the information k0rdent needs to know in order to create the cluste you're looking for. This <code>ClusterDeployment</code> system relies on predefined templates and credentials. </p> <p>A cluster deployment typically involves:</p> <ol> <li>Setting up credentials for the infrastructure provider (for example, AWS, vSphere).</li> <li>Choosing a template that defines the desired cluster configuration (for example, number of nodes, instance types).</li> <li>Submitting the configuration for deployment and monitoring the process.</li> </ol> <p>Follow these steps to deploy a standalone Kubernetes cluster tailored to your specific needs:</p> <ol> <li> <p>Create the <code>Credential</code> object</p> <p>Credentials are essential for k0rdent to communicate with the infrastructure provider (for example, AWS, Azure, vSphere). These credentials enable k0rdent to provision resources such as virtual machines, networking components, and storage.</p> <p><code>Credential</code> objects are generally created haead of time and made available to users, so before you look into creating a new one be sure what you're looking for doesn't already exist. You can see all of the existing <code>Credential</code> obects by  querying the management cluster:</p> <pre><code>kubectl get credentials -n A\n</code></pre> <p>If the <code>Credential</code> you need doesn't yet exist, go ahead and create it.</p> <p>Start by creating a <code>Credential</code> object that includes all required authentication details for your chosen infrastructure provider. Follow the instructions in the chapter about credential management, as well as the specific instructions for your target infrastructure.</p> <p>Tip: Double-check to make sure that your credentials have sufficient permissions to create resources on the target infrastructure.</p> </li> <li> <p>Select a Template</p> <p>Templates in k0rdent are predefined configurations that describe how the cluster should be set up. Templates include details such as: - The number and type of control plane and worker nodes. - Networking settings. - Regional deployment preferences.</p> <p>Templates act as a blueprint for creating a cluster. To see the list of available templates, use the following command:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>You can then get information on the actual template by describing it, as in:</p> <pre><code>kubectl describe clustertemplate aws-standalone-cp-0-0-5 -n ksm-system\n</code></pre> </li> <li> <p>Create a ClusterDeployment YAML Configuration</p> <p>The <code>ClusterDeployment</code> object is the main configuration file that defines your cluster's specifications. It includes: - The template to use. - The credentials for the infrastructure provider. - Optional customizations such as instance types, regions, and networking.</p> <p>Create a <code>ClusterDeployment</code> configuration in a YAML file, following this structure:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\" (default: \"false\")&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> <p>You will of course want to replace the placeholders with actual values. (For more information about <code>dryRun</code> see Understanding the Dry Run.) For example, this is a simple AWS infrastructure provider <code>ClusterDeployment</code>:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: false\n  config:\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> Note that the <code>.spec.credential</code> value should match the <code>.metadata.name</code> value of a created <code>Credential</code> object.</p> </li> <li> <p>Apply the Configuration</p> <p>Once the <code>ClusterDeployment</code> configuration is ready, apply it to the k0rdent management cluster:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> <p>This step submits your deployment request to k0rdent. If you've set <code>dryRun</code> to <code>true</code> you can observe what would happen. Otherwise, k0rdent will go ahead and begin provisioning the necessary infrastructure.</p> </li> <li> <p>Verify Deployment Status</p> <p>After submitting the configuration, verify that the <code>ClusterDeployment</code> object has been created successfully:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>The output shows the current status and any errors.</p> </li> <li> <p>Monitor Provisioning</p> <p>k0rdent will now start provisioning resources (e.g., VMs, networks) and setting up the cluster. To monitor this process, run:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> <p>Tip: For a detailed view of the provisioning process, use the <code>clusterctl describe</code> command (note that this requires the <code>clusterctl</code> CLI):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre> </li> <li> <p>Retrieve the Kubernetes Configuration</p> <p>When provisioning is complete, retrieve the kubeconfig file for the new cluster. This file enables you to interact with the cluster using <code>kubectl</code>:</p> <p><pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> You can then use this file to access the cluster, as in:</p> <pre><code>export KUBECONFIG=kubeconfig\nkubectl get pods -A\n</code></pre> <p>Store the kubeconfig file securely, as it contains authentication details for accessing the cluster.</p> </li> </ol>"},{"location":"admin-creating-clusters/#updating-a-single-standalone-cluster","title":"Updating a Single Standalone Cluster","text":"<p>k0rdent <code>ClusterTemplate</code>s are immutable, so the only way to change a <code>ClusterDeployment</code> is to change the template that forms its basis. </p> <p>To update the <code>ClusterDeployment</code>, modify the <code>.spec.template</code> field to use the name of the new <code>ClusterTemplate</code>.  This enables you to apply changes to the cluster configuration. These changes will then be applied to the actual  cluster. For example, if the cluster currently uses <code>t2.large</code> instances, that will be specified in its current template.  To change the cluster to use <code>t2.xlarge</code> instances, you would simply apply a template that references that new size;  k0rdent will then realize the cluster is out of sync and will attempt to remedy the situation by updating the cluster.</p> <p>Follow these steps to update the <code>ClusterDeployment</code>:</p> <ol> <li>Patch the <code>ClusterDeployment</code> with the new template:</li> </ol> <p>Run the following command, replacing the placeholders with the appropriate values:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> <ol> <li>Check the status of the <code>ClusterDeployment</code>:</li> </ol> <p>After applying the patch, verify the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> <ol> <li>Inspect the detailed status:</li> </ol> <p>For more details, use the <code>-o=yaml</code> option to check the <code>.status.conditions</code> field:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; -o=yaml\n</code></pre> <p>Note that not all updates are possible; <code>ClusterTemplateChain</code> objects limit what templates can be applied.  Consider, for example, this <code>ClusterTemplateChain</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws-standalone-cp-0.0.2\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0.0.1\n      availableUpgrades:\n        - name: aws-standalone-cp-0.0.2\n    - name: aws-standalone-cp-0.0.2\n</code></pre> <p>As you can see from the <code>.spec</code>, the <code>aws-standalone-co-0.0.2</code> template can be applied to a cluster that also uses the <code>aws-standalone-co-0.0.2</code> template, or it can be used as an upgrade from a cluster that uses <code>aws-standalone-co-0.0.1</code>. You wouldn't be able to use this template to update a cluster that uses any other <code>ClusterTemplate</code>.</p> <p>Similarly, the <code>AccessManagement</code> object must have properly configured <code>spec.accessRules</code> with a list of allowed  <code>ClusterTemplateChain</code> object names and their namespaces. For more information, see Template Life Cycle Management.</p> <p>Future Note: Support for displaying all available <code>ClusterTemplates</code> for updates in the <code>ClusterDeployment</code> status is planned.</p>"},{"location":"admin-creating-clusters/#scaling-a-cluster","title":"Scaling a Cluster ,","text":""},{"location":"admin-creating-clusters/#upgrading-a-single-standalone-cluster","title":"Upgrading a Single Standalone Cluster ,","text":""},{"location":"admin-creating-clusters/#cleanup","title":"Cleanup","text":"<p>Especially when you are paying for cloud resources, it's crucial to clean up when you're finished. Fortunately, k0rdent makes that straightforward.</p> <p>Because a Kubernetes cluster is represented by a <code>ClusterDeployment</code>, when you delete that <code>ClusterDeployment</code>, k0rdent deletes the cluster. For example:</p> <p><pre><code>kubectl delete clusterdeployment my-cluster-deployment -n kmc-system\n</code></pre> <pre><code>ClusterDeployment my-cluster-deployment deleted.\n</code></pre></p> <p>It takes time to delete these resources.</p>"},{"location":"admin-credentials/","title":"Credentials","text":""},{"location":"admin-hosted-control-planes/","title":"Deploying a Hosted Control Plane","text":"<p>A hosted control plane is a Kubernetes setup in which the control plane components (such as the API server,  etcd, and controllers) run inside the management cluster instead of separate controller nodes. This  architecture centralizes control plane management, and improves scalability by sharing resources in the management cluster. Hosted control planes are managed by k0smotron.</p> <p>Instructions for setting up a hosted control plane vary slighting depending on the provider.</p>"},{"location":"admin-hosted-control-planes/#aws-hosted-control-plane-deployment","title":"AWS Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on AWS: </p> <ol> <li> <p>Prerequisites</p> <p>Before proceeding, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28 or later) deployed on AWS with k0rdent installed.</li> <li>A default storage class configured on the management cluster to support Persistent Volumes.</li> <li>The VPC ID where the worker nodes will be deployed.</li> <li>The Subnet ID and Availability Zone (AZ) for the worker nodes.</li> <li>The AMI ID for the worker nodes (Amazon Machine Image ID for the desired OS and Kubernetes version).</li> </ul> <p>Important: All control plane components for your hosted cluster will reside in the management cluster. The management cluster    must have sufficient resources to handle these additional workloads.</p> </li> <li> <p>Networking</p> <p>To deploy a hosted control plane, the necessary AWS networking resources must already exist or be created. If you're  using the same VPC and subnets as your management cluster, you can resuse these resources.</p> <p>If your management cluster was deployed using the Cluster API Provider AWS (CAPA), you can gather the required  networking details using the following commands:</p> <p>Retrieve the VPC ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre></p> <p>Retrieve Subnet ID: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre></p> <p>Retrieve Availability Zone: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre></p> <p>Retrieve Security Group: <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>Retrieve AMI ID: <pre><code>kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre></p> <p>Tip: If you want to use different VPCs or regions for your management and hosted clusters, you\u2019ll need to configure additional networking, such as VPC peering, to allow communication between them.</p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>Once you've collected all the necessary data, you can create the <code>ClusterDeployment</code> manifest. This file tells k0rdent how to  deploy and manage the hosted control plane. Below is an example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-0-3\n  credential: aws-credential\n  config:\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note: The example above uses the <code>us-west-1</code> region, but you should use the region of your VPC.</p> </li> <li> <p>Apply the <code>ClusterTemplate</code></p> <p>Nothing actually happens until you apply the <code>ClusterDeployment</code> manifest to create a new cluster deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml -n kcm-system\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#generate-the-clusterdeployment-manifest","title":"Generate the <code>ClusterDeployment</code> Manifest","text":"<p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dyamically  inserts the appropriate values:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-0-3\n  credential: aws-credential\n  config:\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Save this template as <code>clusterdeployment.yaml.tpl</code>, then generate your manifest using the following command:</p> <pre><code>kubectl get awscluster &lt;cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre>"},{"location":"admin-hosted-control-planes/#deployment-tips","title":"Deployment Tips","text":"<p>Here are some additional tips to help with deployment:</p> <ol> <li>Controller and Template Availability:</li> </ol> <p>Make sure the kcm controller image and templates are available in a public or accessible repository.</p> <ol> <li>Install Charts and Templates:</li> </ol> <p>If you're using a custom repository, run the following commands with the appropriate <code>kubeconfig</code>:</p> <pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> <ol> <li>Mark Infrastructure as Ready:</li> </ol> <p>To scale up the <code>MachineDeployment</code>, manually mark the infrastructure as ready:    <pre><code>kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}' -n kcm-system\n</code></pre>    For more details on why this is necessary, click here.</p>"},{"location":"admin-hosted-control-planes/#azure-hosted-control-plane-deployment","title":"Azure Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on Azure:</p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on Azure with k0rdent installed.</li> <li>A default storage class configured    on the management cluster to support Persistent Volumes.</li> </ul> <p>Note: All control plane components for managed clusters will run in the management cluster. Make sure the management cluster    has sufficient CPU, memory, and storage to handle the additional workload.</p> </li> <li> <p>Gather pre-existing resources</p> <p>In a hosted control plane setup, some Azure resources must exist before deployment and must be explicitly  provided in the <code>ClusterDeployment</code> configuration. These resources can also be reused by the management cluster.</p> <p>If you deployed your Azure Kubernetes cluster using the Cluster API Provider for Azure (CAPZ), you can retrieve  the required information using the following commands:</p> <p>Location: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre></p> <p>Subscription ID: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre></p> <p>Resource Group: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre></p> <p>VNet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre></p> <p>Subnet Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre></p> <p>Route Table Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre></p> <p>Security Group Name: <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre></p> </li> <li> <p>Create the ClusterDeployment manifest</p> <p>After collecting the required data, create a <code>ClusterDeployment</code> manifest to configure the hosted control plane. It should look something like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> </li> <li> <p>Create the <code>ClusterDeployment</code></p> <p>To actually create the cluster, apply the <code>ClusterDeployment</code> manifest to the management cluster, as in:</p> <pre><code>kubectl apply clusterdeployment.yaml -n kcm-system\n</code></pre> </li> <li> <p>Manually update the <code>AzureCluster</code> object</p> <p>Due to a limitation on k0smotron, (see k0sproject/k0smotron#668),  after applying the <code>ClusterDeployment</code> manifest, you must manually update the status of the <code>AzureCluster</code> object.</p> <p>Use the following command to set the <code>AzureCluster</code> object status to <code>Ready</code>:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch '{\"status\": {\"ready\": true}}'\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#generate-a-clusterdeployment-manifest","title":"Generate a <code>ClusterDeployment</code> Manifest","text":"<p>To simplify the creation of a <code>ClusterDeployment</code> manifest, you can use the following template, which dyamically inserts  the appropriate values:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Save this YAML as <code>clusterdeployment.yaml.tpl</code> and render the manifest with the following command: <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre></p>"},{"location":"admin-hosted-control-planes/#important-notes-on-cluster-deletion","title":"Important Notes on Cluster Deletion","text":"<p>Due to these same k0smotron limitations, you must take some manual steps in order to delete a cluster properly:</p> <ol> <li>Add a Custom Finalizer to the AzureCluster Object:</li> </ol> <p>To prevent the <code>AzureCluster</code> object from being deleted too early, add a custom finalizer:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": [\"manual\"]}}'\n</code></pre> <ol> <li> <p>Delete the ClusterDeployment:    After adding the finalizer, delete the <code>ClusterDeployment</code> object as usual. Confirm that all <code>AzureMachines</code> objects have been deleted successfully.</p> </li> <li> <p>Remove Finalizers from Orphaned AzureMachines:</p> </li> </ol> <p>If any <code>AzureMachines</code> are left orphaned, delete their finalizers manually after confirming no VMs remain in Azure. Use this command to remove the finalizer:</p> <pre><code>kubectl patch azuremachine &lt;machine-name&gt; --type=merge --patch '{\"metadata\": {\"finalizers\": []}}'\n</code></pre> <ol> <li> <p>Allowing Updates to Orphaned Objects:</p> <p>If Azure admission controls prevent updates to orphaned objects, you must disable the associated <code>MutatingWebhookConfiguration</code> by deleting it:</p> <pre><code>kubectl delete mutatingwebhookconfiguration &lt;webhook-name&gt;\n</code></pre> </li> </ol>"},{"location":"admin-hosted-control-planes/#vsphere-hosted-control-plane-deployment","title":"vSphere Hosted Control Plane Deployment","text":"<p>Follow these steps to set up a k0smotron-hosted control plane on vSphere. </p> <ol> <li> <p>Prerequisites</p> <p>Before you start, make sure you have the following:</p> <ul> <li>A management Kubernetes cluster (Kubernetes v1.28+) deployed on vSphere with k0rdent installed.</li> </ul> <p>All control plane components for managed clusters will reside in the management cluster. Make sure the management  cluster has sufficient resources (CPU, memory, and storage) to handle these workloads.</p> </li> <li> <p>Create the <code>ClusterDeployment</code> Manifest</p> </li> </ol> <p>The <code>ClusterDeployment</code> manifest for vSphere-hosted control planes is similar to standalone control plane deployments.  For a detailed list of parameters, refer to the template parameters section.</p> <p>Important: The vSphere provider requires you to specify the control plane endpoint IP before deploying the cluster. This IP  address must match the one assigned to the k0smotron load balancer (LB) service. Use an annotation supported by your load balancer provider to assign the control plane endpoint IP to the k0smotron  service. For example, the manifest below includes a <code>kube-vip</code> annotation.</p> <p><code>ClusterDeployment</code>s for vSphere-based clusters include a <code>.spec.config.vsphere</code> object that contains vSphere-specific parameters. For example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre> <p>For more information on these parameters, see the Template Reference. </p>"},{"location":"admin-installation/","title":"Installing k0rdent","text":"<p>The process of installing k0rdent is straightforward, and involves the following steps:</p> <ol> <li>Create a Kubernetes cluster to act as the management cluster</li> <li>Install k0rdent into the management cluster</li> <li>Add the necessary credentials and templates to work with the providers in your infrastructure.</li> </ol>"},{"location":"admin-installation/#create-and-prepare-the-management-kubernetes-cluster","title":"Create and prepare the Management Kubernetes cluster","text":"<p>The first step is to create the Kubernetes cluster.</p>"},{"location":"admin-installation/#cncf-certified-kubernetes","title":"CNCF-certified Kubernetes","text":"<p>k0rdent is designed to run on any CNCF-certified Kubernetes. This gives you great freedom in setting up k0rdent for convenient, secure, and reliable operations.</p> <ul> <li>Proof-of-concept scale k0rdent implementations can run on a beefy Linux desktop or laptop, using a single-node-capable CNCF Kubernetes distribution like k0s, running natively as bare processes, or inside a container with KinD.</li> <li>More capacious implementations can run on multi-node Kubernetes clusters on bare metal or in clouds.</li> <li>Production users can leverage cloud provider Kubernetes variants like Amazon EKS or Azure AKS to quickly create highly-reliable and scalable k0rdent management clusters.</li> </ul>"},{"location":"admin-installation/#mixed-use-management-clusters","title":"Mixed-use management clusters","text":"<p>k0rdent management clusters can also be mixed-use. For example, a cloud Kubernetes k0rdent management cluster can also be used as a 'mothership' environment for k0smotron hosted control planes managed by k0rdent, integrated with workers bootstrapped (also by k0rdent) on adjacent cloud VMs or on remote VMs, bare metal servers, or edge nodes.</p>"},{"location":"admin-installation/#where-k0rdent-management-clusters-can-live","title":"Where k0rdent management clusters can live","text":"<p>k0rdent management clusters can live anywhere, so long as they are connected via internet to the clusters they manage. Unlike prior generations of Kubernetes cluster managers, there's no technical need to co-locate a k0rdent manager with managed clusters on a single infrastructure. In fact, k0rdent is being built to manage multiple IDPs and platforms, providing a single point of control and visibility. Deciding where to put a management cluster (or multiple clusters) is best done by assessing the requirements of your use case. Considered in the abstract, a k0rdent management cluster should be:</p> <ul> <li>Resilient and available</li> <li>Accessible and secure</li> <li>Easy to network</li> <li>Scalable (particularly for mixed-use implementations)</li> <li>Easy to operate with minimum overhead</li> <li>Monitored and observable</li> <li>Equipped for backup and disaster recovery</li> </ul> <p>And of course, where can these requirements be procured at reasonable cost? In many cases the simplest and most cost-effective way to run these clusters is on a provider such as Amazon Elasti Kubernetes Service or Azure Kubernetes Service, where many fo these functions are handled for you.</p>"},{"location":"admin-installation/#minimum-requirements","title":"Minimum requirements","text":"<p>[[[ TODO ]]]</p>"},{"location":"admin-installation/#recommended-requirements-for-production","title":"Recommended requirements for production","text":"<p>[[[ TODO ]]]</p>"},{"location":"admin-installation/#create-and-prepare-a-kubernetes-cluster-with-k0s","title":"Create and prepare a Kubernetes cluster with k0s","text":"<p>If you already have a Kubernetes cluster into which you want to install k0rdent, skip to the next step. Otherwise, follow these steps to install and prepare a k0s kubernetes management cluster:</p> <ol> <li> <p>Deploy a Kubernetes cluster</p> <p>The first step is to create the actual cluster itself. Again, the actual distribution used for the management cluster isn't important, as long as it's a CNCF-compliant distribution. That means you can use an existing EKS cluster, or whatever is your normal corporate standard. To make things simple this guide uses k0s, a small, convenient, and fully-functional distribution:</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>k0s includes its own preconfigured version of <code>kubectl</code> so make sure the cluster is running:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see a single node with a status of <code>Ready</code>, as in:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre> </li> <li> <p>Install kubectl</p> <p>Everything you do in k0rdent is done by creating and manipulating Kubernetes objects, so you'll need to have <code>kubectl</code> installed:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre> </li> <li> <p>Get the kubeconfig</p> <p>In order to access the management cluster you will, of course, need the kubeconfig. Again, if you're using another Kubernetes distribution follow those instructions to get the kubeconfig, but for k0s, the process involves simply copying the existing file and adding it to an environment varable so <code>kubectl</code> knows where to find it.</p> <p><pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> Now you should be able to use the non-k0s <code>kubectl</code> to see the status of the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Again, you should see the single k0s node, but by this time it should have had its role assigned, as in:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre> <p>Now the cluster is ready for installation, which we'll do using Helm.</p> </li> <li> <p>Install Helm</p> <p>The easiest way to install k0rdent is through its Helm chart, so let's get Helm installed:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Helm will be installed into <code>/usr/local/bin/helm</code>.</p> </li> </ol>"},{"location":"admin-installation/#install-k0rdent","title":"Install k0rdent","text":"<p>The actual management cluster is a Kubernetes cluster with the k0rdent application installed. The simplest way to install k0rdent is through its Helm chart.  You can find the latest release here, and from there you can deploy the Helm chart, as in:</p> <p><pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/mirantis/hmc/charts/hmc:0.0.3\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: hmc\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: hmc-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p> <p>(Make sure to specify the correct version number.)</p> <p>While the installation process appears to be done at this point, it's not; multiple pods and templates are being created in the background, and the entire process takes a few minutes.  </p> <p>To understand whether installation is complete, start by making sure all pods are ready in the <code>kcm-system</code> namespace. There should be 15:</p> <p><pre><code>kubectl get pods -n kcm-system\n</code></pre> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre></p> <p>State management is handled by Project Sveltos, so you'll want to make sure that all 9 pods are running in the <code>projectsveltos</code> namespace:</p> <p><pre><code>kubectl get pods -n projectsveltos\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre></p> <p>If any of these pods are missing, simply give k0rdent more time. If there's a problem, you'll see pods crashing and restarting, and you can see what's happening by describing the pod, as in:</p> <pre><code>kubectl describe pod classifier-manager-574c9d794d-j8852 -n projectsveltos\n</code></pre> <p>As long as you're not seeing pod restarts, you just need to wait a few minutes.</p> <p>Next verify whether the kcm templates have been successfully installed and reconciled.  Start with the <code>ProviderTemplate</code> objects:</p> <p><pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                   VALID\ncluster-api-0-0-6                      true\ncluster-api-provider-aws-0-0-4         true\ncluster-api-provider-azure-0-0-4       true\ncluster-api-provider-openstack-0-0-1   true\ncluster-api-provider-vsphere-0-0-5     true\nk0smotron-0-0-6                        true\nkcm-0-0-7                              true\nprojectsveltos-0-45-0                  true\n</code></pre></p> <p>Make sure that all templates are not just installed, but valid. Again, this may take a few minutes.</p> <p>You'll also want to make sure the <code>ClusterTemplate</code> objects are installed and valid:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAME                                VALID\nadopted-cluster-0-0-2               true\naws-eks-0-0-3                       true\naws-hosted-cp-0-0-4                 true\naws-standalone-cp-0-0-5             true\nazure-aks-0-0-2                     true\nazure-hosted-cp-0-0-4               true\nazure-standalone-cp-0-0-5           true\nopenstack-standalone-cp-0-0-2       true\nvsphere-hosted-cp-0-0-5             true\nvsphere-standalone-cp-0-0-5         true\n</code></pre></p> <p>Finally, make sure the <code>ServiceTemplate</code> objects are installed and valid:</p> <p><pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre></p>"},{"location":"admin-installation/#backing-up-a-k0rdent-management-cluster","title":"Backing up a k0rdent management cluster","text":"<p>In a production environment, you will always want to ensure that your management cluster is backed up. You can back it up just  as you would any other cluster, or you can prepare your cluster for use Velero as a backup provider.</p>"},{"location":"admin-prepare/","title":"Prepare k0rdent to create managed clusters on multiple providers","text":"<p>Managed clusters can be hosted on a number of different platforms. At the time of this writing, those platforms include:</p> <ul> <li>Amazon Web Services</li> <li>Microsoft Azure</li> <li>OpenStack</li> <li>VMware</li> </ul>"},{"location":"admin-prepare/#aws","title":"AWS","text":"<p>k0rdent is able to deploy managed clusters as both EC2-based Kubernetes clusters and EKS clusters. In both cases, you'll need to create the relevant credentials, and to do that you'll need to configure an IAM user. Follow these steps to make it possible to deploy to AWS:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>Install <code>clusterawsadm</code></p> <p>k0rdent uses the Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. clusterawsadm, a CLI tool created by CAPA project, helps with AWS-specific tasks such as creating IAM roles and policies, as well as credential configuration. To install clusterawsadm on Ubuntu on x86 hardware, execute these commands:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre> </li> <li> <p>Configure AWS IAM</p> <p>Next you'll need to create the IAM policies and service account k0rdent will use to take action within the AWS infrastructure. (Note that you only need to do this once.)</p> <p>The first step is to crete the IAM CloudFormation stack based on your admin user. Start by specifying the environment variables clusterawsadmin will use as AWS credentials:</p> <pre><code>export AWS_REGION=&lt;EXAMPLE_AWS_REGION&gt;\nexport AWS_ACCESS_KEY_ID=&lt;EXAMPLE_ACCESS_KEY_ID&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;EXAMPLE_SECRET_ACCESS_KEY&gt;\nexport AWS_SESSION_TOKEN=&lt;YOUR_SESSION_TOKEN&gt; # Optional. If you are using Multi-Factor Auth.\n</code></pre> </li> <li> <p>Create the IAM CloudFormation </p> <p>Now use <code>clusterawsadm</code> to create the IAM CloudFormation stack:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre> </li> <li> <p>Install the AWS CLI</p> <p>With the stack in place you can create the AWS IAM user. You can do this in the UI, but it's also possible to do it from the command line using the aws CLI tool.  Start by installing it, if you haven't already:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre> <p>The tool recognizes the environment variables you created earlier, so there's no need to login.</p> </li> <li> <p>Create the IAM user. </p> <p>The actual <code>user-name</code> parameter is arbitrary; you can specify it as anything you like:</p> <p><pre><code>aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <pre><code>{\n  \"User\": {\n      \"Path\": \"/\",\n      \"UserName\": \"k0rdentQuickstart\",\n      \"UserId\": \"EXAMPLE_USER_ID\",\n      \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n      \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Assign the relevant policies</p> <p>You'll need to assign the following policies to the user you just created:</p> <p><pre><code>control-plane.cluster-api-provider-aws.sigs.k8s.io\ncontrollers.cluster-api-provider-aws.sigs.k8s.io\nnodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> To do that, you'll need the ARNs for each policy.  You can get them with the <code>list-policies</code> command, as in:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> <pre><code>{\n  \"Policies\": [\n      {\n          \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n      },\n      {\n          \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 3,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      },\n      {\n          \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n          \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n          \"Path\": \"/\",\n          \"DefaultVersionId\": \"v1\",\n          \"AttachmentCount\": 2,\n          \"PermissionsBoundaryUsageCount\": 0,\n          \"IsAttachable\": true,\n          \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n          \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n      }\n  ]\n}\n</code></pre></p> <p>Now you can add the policies using the <code>attach-user-policy</code> command and the ARNs you retrieved in the previous step:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> </li> <li> <p>Create an access key and secret</p> <p>To access AWS as this new user, you'll need to create an access key:</p> <p><pre><code>aws iam create-access-key --user-name k0rdentQuickstart \n</code></pre> <pre><code>{\n  \"AccessKey\": {\n      \"UserName\": \"k0rdentQuickstart\",\n      \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n      \"Status\": \"Active\",\n      \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n      \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n  }\n}\n</code></pre></p> </li> <li> <p>Create the IAM Credentials Secret on the k0rdent Management Cluster</p> <p>Create a YAML file called aws-cluster-identity-secret.yaml and add the following text, including the <code>AccessKeyId</code> and <code>SecretAccessKey</code> you created in the previous step:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\ntype: Opaque\nstringData:\n  AccessKeyID: EXAMPLE_ACCESS_KEY_ID\n  SecretAccessKey: EXAMPLE_SECRET_ACCESS_KEY\n</code></pre> <p>Apply the YAML to your cluster, making sure to add it to the namespace where the CAPA provider is running (currently <code>kcm-system</code>) so the controller can read it:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>AWSClusterStaticIdentity</code></p> <p>Create the <code>AWSClusterStaticIdentity</code> object in a file named <code>aws-cluster-identity.yaml</code>:</p> <pre><code>kind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Notice that the <code>secretRef</code> references the <code>Secret</code> you created in the previous step.</p> <p>Apply the YAML to your cluster, again adding it to the <code>kcm-system</code> namespace.</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre> </li> <li> <p>Create the <code>Credential</code></p> <p>Finally, create the kcm <code>Credential</code> object, making sure to reference the <code>AWSClusterStaticIdentity</code> you just created:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\ndescription: \"Credential Example\"\nidentityRef:\n  apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n  kind: AWSClusterStaticIdentity\n  name: aws-cluster-identity\n</code></pre> Apply the YAML to your cluster, again keeping it in the <code>kcm-system</code> namespace:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre> </li> <li> <p>Deploy a cluster</p> <p>Make sure everything is configured properly by creating a <code>ClusterDeployment</code>. Start with a YAML file specifying the <code>ClusterDeployment</code>, as in:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>A couple of things to notice: - You're giving it an arbitrary name in <code>.metadata.name</code> (<code>my-aws-clusterdeployment1</code>) - You're referencing the credential you created in the previous step, <code>aws-cluster-identity-cred</code>. This enables you to set up a system where users can take advantage of having access to the credentials to the AWS account without actually having those credentials in hand. - You need to choose a template to use for the cluster, in this case <code>aws-standalone-cp-0-0-5</code>. You can get a list of available templates using:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre></p> <p>As before, there will be a delay as the cluster finishes provisioning. Follow the provisioning process with:</p> <p><pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre></p> <p>When the cluster is <code>Ready</code>, you can access it via the kubeconfig, as in:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>When you've established that it's working properly. you can delete the managed cluster and its AWS objects:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 \n</code></pre> </li> </ol>"},{"location":"admin-prepare/#azure","title":"Azure","text":"<p>Standalone clusters can be deployed on Azure instances. Follow these steps to make Azure clusters available to your users:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>The Azure CLI</p> <p>The Azure CLI (az) is required to interact with Azure resources. You can install it on Ubuntu as follows:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> </li> <li> <p>Log in to Azure</p> <p>Run the <code>az login</code> command to authenticate your session with Azure:</p> <pre><code>az login\n</code></pre> <p>Make sure that the account you're using has at least one active subscription.</p> </li> <li> <p>Register resource providers</p> <p>In order for k0rdent to deploy and manage clusters, it needs to be able to work with Azure resources such as  compute, network, and identity. Make sure the subscription you're using has the following resource providers registered:</p> <ul> <li><code>Microsoft.Compute</code></li> <li><code>Microsoft.Network</code></li> <li><code>Microsoft.ContainerService</code></li> <li><code>Microsoft.ManagedIdentity</code></li> <li><code>Microsoft.Authorization</code></li> </ul> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> </li> <li> <p>Find Your Subscription ID</p> <p>Creating a managed cluster requires a structure of credentials that link to user identities on the provider system without exposing the actual username and password to users. You can find more information on k0rdent  Credentials, but for Azure, this involves creating an <code>AzureClusterIdentity</code> and a  Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with the cloud provider. </p> <p>On Azure, the lowest level of this hierarchy is the subscription, which ties to your billing information Azure.Your Azure must have at least one subscription for you to use it with k0rdent, so if you're working with a new account make sure to create a new subscription with billing information before you start.</p> <p>To get the information you need, list all your Azure subscriptions: </p> <p><pre><code>az account list -o table\n</code></pre> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID        TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre></p> <p>Make note of the <code>SubscriptionId</code> for the subscription you want to use.</p> </li> <li> <p>Create a Service Principal (SP)</p> <p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. Create the Service Principal, making sure to replace  with the <code>SubscriptionId</code> from step 1. <p><pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt;\"\n</code></pre> <pre><code>{\n\"appId\": \"SP_APP_ID_SP_APP_ID\",\n\"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n\"password\": \"SP_PASSWORD_SP_PASSWORD\",\n\"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> Note that this information gives you access to your Azure account, so make sure to treat these strings  like passwords. Do not share them or check them into a repository.</p> <li> <p>Use the password to create a <code>Secret</code> object</p> <p>The <code>Secret</code> stores the <code>clientSecret</code> (password) from the Service Principal. Save the <code>Secret</code> YAML in a file called <code>azure-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: &lt;SP_PASSWORD_SP_PASSWORD&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>You can then apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>AzureClusterIdentity</code> objects</p> <p>The <code>AzureClusterIdentity</code> object defines the credentials CAPZ uses to manage Azure resources.  It references the <code>Secret</code> you just created, so make sure that <code>.spec.clientSecret.name</code> matches  the name of that <code>Secret</code>.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;SP_APP_ID_SP_APP_ID&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;SP_TENANT_SP_TENANT&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre> <pre><code>azureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p> </li> <li> <p>Create the k0rdent <code>Credential</code> Object</p> <p>Create the YAML for the specification of the <code>Credential</code> and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>You're referencing the <code>AzureClusterIdentity</code> object you just created, so make sure that <code>.spec.name</code> matches  <code>.metadata.name</code> of that object. Also, note that while the overall object's <code>kind</code> is <code>Credential</code>, the  <code>.spec.identityRef.kind</code> must be <code>AzureClusterIdentity</code> to match that object.</p> <p>Apply the YAML to your cluster:</p> <p><pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre></p> </li> <p>Now you're ready to deploy the cluster.</p> <ol> <li> <p>Create a <code>ClusterDeployment</code></p> <p>To test the configuration, deploy a managed cluster by following these steps:</p> <p>First get a list of available locations/regions:</p> <p><pre><code>az account list-locations -o table\n</code></pre> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n\u2026\n</code></pre></p> <p>Make note of the location you want to use, such as <code>eastus</code>.</p> <p>To create the actual managed cluster, create a <code>ClusterDeployment</code> that references the appropriate template as well as the location, credentials, and <code>subscriptionId</code>.</p> <p>You can see the available templates by listing them:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>Create the yaml:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;SUBSCRIPTION_ID_SUBSCRIPTION_ID&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> <p>Apply the YAML to your management cluster:</p> <p><pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre></p> <p>Note that although the <code>ClusterDeployment</code> object has been created, there will be a delay as actual Azure instances are provisioned and added to the cluster. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>If the provisioning process continues for a more than a few minutes, check to make sure k0rdent isn't trying to exceed your quotas. If you are near the top of your quotas, requesting an increase can \"unstick\" the provisioning process.</p> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up Azure resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#openstack","title":"OpenStack","text":"<p>k0rdent is able to deploy managed clusters on OpenStack virtual machines. Follow these steps to make it possible to deploy to OpenStack:</p> <ol> <li> <p>Install k0rdent</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running.</p> </li> <li> <p>OpenStack CLI (optional)</p> <p>If you plan to access OpenStack directly, go ahead and  install the OpenStack CLI.</p> </li> <li> <p>Configure the OpenStack Application Credential</p> <p>This credential should include:</p> <ul> <li><code>OS_AUTH_URL</code></li> <li><code>OS_APPLICATION_CREDENTIAL_ID</code></li> <li><code>OS_APPLICATION_CREDENTIAL_SECRET</code></li> <li><code>OS_REGION_NAME</code></li> <li><code>OS_INTERFACE</code></li> <li><code>OS_IDENTITY_API_VERSION</code> (commonly <code>3</code>)</li> <li><code>OS_AUTH_TYPE</code> (e.g., <code>v3applicationcredential</code>)</li> </ul> <p>While it's possible to use a username and password instead of the Application Credential--adjust your YAML accordingly--an  Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p> </li> <li> <p>Create the OpenStack Credentials Secret</p> <p>Create a Kubernetes <code>Secret</code> containing the <code>clouds.yaml</code> that defines your OpenStack environment, substituting real values where appropriate. Save this as <code>openstack-cloud-config.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre> </li> <li> <p>Create the k0rdent Credential Object</p> <p>Next, define a <code>Credential</code> that references the <code>Secret</code> from the previous step. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note that <code>.spec.identityRef.name</code> must match the <code>Secret</code> you created in the previous step, and  <code>.spec.identityRef.namespace</code> must be the same as the <code>Secret</code>\u2019s namespace (<code>kcm-system</code>).</p> </li> <li> <p>Create Your First Managed Cluster</p> <p>To test the configuration, create a YAML file with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.  Note that you can see the available templates by listing them:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> should look something like this:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-1\n  credential: openstack-cluster-identity-cred\n  config:\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: &lt;OS_AUTH_URL&gt;\n</code></pre> You can adjust <code>flavor</code>, <code>image</code> name, and <code>authURL</code> to match your OpenStack environment. For more information about the configuration options, see the OpenStack Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can follow the provisioning process:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, just like any other Kubernetes cluster:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value&gt;' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\nKUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To clean up OpenStack resources, delete the managed cluster by deleting the <code>ClusterDeployment</code>:</p> <p><pre><code>kubectl get ClusterDeployments -A\n</code></pre> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-openstack-cluster-deployment   True    ClusterDeployment is ready\n</code></pre> <pre><code>kubectl delete ClusterDeployment my-openstack-cluster-deployment -n kcm-system\n</code></pre> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-openstack-cluster-deployment\" deleted\n</code></pre></p> </li> </ol>"},{"location":"admin-prepare/#vsphere","title":"vSphere","text":"<p>To enable users to deploy managed clusers on vSphere, follow these steps:</p> <ol> <li> <p>Create a k0rdent management cluster</p> <p>Follow the instructions in Install k0rdent to create a management cluster with k0rdent running, as well as a local install of <code>kubectl</code>.</p> </li> <li> <p>Install a vSphere instance version <code>6.7.0</code> or higher.</p> </li> <li> <p>Create a vSphere account with appropriate privileges</p> <p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The user should have the following  required privileges:</p> <ul> <li><code>Virtual machine</code>: Full permissions are required</li> <li><code>Network</code>: <code>Assign network</code> is sufficient</li> <li><code>Datastore</code>: The user should be able to manipulate virtual machine files and metadata</li> </ul> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p> </li> <li> <p>Image template</p> <p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p> </li> <li> <p>vSphere network</p> <p>When creating a network, make sure that it has the DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (for example, the network <code>172.16.0.0/24</code> should have a DHCP range of <code>172.16.0.100-172.16.0.254</code> only) so that LoadBalancer services will not create any IP conflicts in the network.</p> </li> <li> <p>To enable k0rdent to access vSphere resources, create the appropriate credentials objects. For a full explanation of how Credentials work, see the main Credentials chapter but for now, follow these steps:</p> <p>Create a <code>Secret</code> Object with the username and password</p> <p>The <code>Secret</code> stores the username and password for your vSphere instance. Save the <code>Secret</code> YAML in a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  username: &lt;USERNAME&gt;\n  password: &lt;PASSWORD&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre> </li> <li> <p>Create the <code>VSphereClusterIdentity</code> Object</p> <p>The <code>VSphereClusterIdentity</code> object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the <code>VSphereClusterIdentity</code> YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>The <code>VSpehereClusterIdentity</code> object references the <code>Secret</code> you created in the previous step, so <code>.spec.secretName</code>  needs to match the <code>.metadata.name</code> for the <code>Secret</code>.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre> </li> <li> <p>Create the <code>Credential</code> Object</p> <p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n</code></pre> Again, <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object you just created.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre> </li> <li> <p>Create your first Cluster Deployment</p> <p>Test the configuration by deploying a cluster. Create a YAML document with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>You can get a list of available templates:</p> <p><pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre></p> <p>The <code>ClusterDeployment</code> YAML file should look something like this. Make sure the replace the placeholders with your specific information:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-0-0-5\n  credential: vsphere-cluster-identity-cred\n  config:\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>For more information about the available configuration options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\nKUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> </li> <li> <p>Cleanup</p> <p>To delete the provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre> </li> </ol>"},{"location":"appendices/","title":"Appendices","text":""},{"location":"appendices/#understanding-the-dry-run","title":"Understanding the dry run","text":"<p>Text here.</p>"},{"location":"glossary/","title":"k0rdent Glossary","text":""},{"location":"glossary/#beach-head-services","title":"Beach-head Services","text":"<p>definition here</p>"},{"location":"guide-to-quickstarts/","title":"Guide to QuickStarts","text":"<p>The following QuickStart chapters provide a recipe for quickly installing and trying k0rdent. Setting up k0rdent for production is detailed in the Administrator Guide.</p>"},{"location":"guide-to-quickstarts/#what-the-quickstart-covers","title":"What the QuickStart covers","text":"<p>The goal of the QuickStart is:</p> <ul> <li>To get a working environment set up for managing k0rdent.</li> <li>To get a Kubernetes cluster and other tools set up for hosting k0rdent itself.</li> <li>To select a cloud environment (AWS or Azure) and configure k0rdent to lifecycle manage clusters on this substrate.</li> <li>To use k0rdent to deploy a managed cluster.</li> <li>(Optional stretch goal) It's also possible to set up the k0rdent management cluster to lifecycle manage clusters on both cloud environments.</li> </ul>"},{"location":"guide-to-quickstarts/#where-the-quickstart-leads","title":"Where the QuickStart leads","text":"<p>The QuickStart shows and briefly explains the hows, whys, and wherefores of manually setting up k0rdent for use. Once built and validated, the QuickStart setup can be leveraged to begin an expanding sequence of demos that let you explore k0rdent's many features. The demos presently use makefiles to speed and simplify setup and operations. We strongly recommend exploring this fast-evolving resource.</p>"},{"location":"guide-to-quickstarts/#quickstart-prerequisites","title":"QuickStart Prerequisites","text":"<p>QuickStart prerequisites are simple \u2014 you'll need:</p> <ul> <li>A desktop or cloud virtual machine running a supported version of Ubuntu Server (e.g., 22.04.5 LTS, Jammy Jellyfish) \u2014 This machine will be used to install a basic Kubernetes working environment, and to host a single-node k0s Kubernetes management cluster to host k0rdent components. For simplest setup, configure this machine as follows:<ul> <li>A minimum of 32GB RAM, 8 vCPUs, 100GB SSD (e.g., AWS <code>t3.2xlarge</code> or equivalent)</li> <li>Set up for SSH access using keys (standard for cloud VMs)</li> <li>Set up for passwordless sudo (i.e., edit /etc/sudoers to configure your user to issue sudo commands without a password challenge)</li> <li>Inbound traffic - SSH (port 22) and ping from your laptop's IP address</li> <li>Outbound traffic - All to any IP address</li> <li>Apply all recent updates and upgrade local applications (sudo apt update/sudo apt upgrade)</li> <li>(Optional) snapshot the machine in its virgin state</li> </ul> </li> <li>Administrative-level access to an AWS or Azure cloud account - Depending on which cloud environment you prefer. k0rdent will leverage this cloud to provide infrastructure for hosting managed clusters.</li> </ul> <p>Note: Ubuntu is a Debian distro and uses <code>apt</code> for package management. Other recent versions of 'enterprise' Linux should work with the following instructions as well, though you will need to adapt for different package managers and perhaps use slightly-different provider-recommended methods for installing required dependencies (e.g., Docker). Once you've installed k0rdent in the management cluster and have kubectl, Helm, and other resources connected, you'll mostly be dealing with Kubernetes, and everything should work the same way on any host OS.</p>"},{"location":"guide-to-quickstarts/#limitations","title":"Limitations","text":"<p>This QuickStart guides you in quickly creating a minimal k0rdent working environment. Setting up k0rdent for production is detailed in the Administrator Guide.</p> <p>The current QuickStart focuses on AWS and Amazon cloud environments, and guides in creating 'standalone' clusters \u2014 in k0rdent parlance, that means 'CNCF-certified Kubernetes clusters with control planes and workers hosted on cloud virtual machines.' The 'CNCF-certified Kubernetes cluster' is the k0s Kubernetes distro.</p> <p>However: this is far from being all that k0rdent can do today. So ...</p>"},{"location":"guide-to-quickstarts/#coming-soon","title":"Coming soon","text":"<p>QuickStarts for other Kubernetes distros, clouds, and environments will appear in the near future (short-term roadmap below):</p> <ul> <li>AWS EKS hosted \u2014 Amazon Elastic Kubernetes Service managed clusters </li> <li>Azure AKS hosted \u2014 Azure Kubernetes Service</li> <li>vSphere standalone \u2014 k0s Kubernetes on vSphere virtual machines</li> <li>OpenStack standalone \u2014 k0s Kubernetes on OpenStack virtual machines</li> </ul> <p>Plus (intermediate-term roadmap) tutorials for using k0rdent to create and manage hybrid, edge, and distributed platforms with Kubernetes-hosted control planes and workers on local or remote substrates.</p> <p>Demo/Tutorials: We will also be converting the demos gradually into tutorials that explain how to use k0rdent for:</p> <ul> <li>Adding services to individual managed clusters, enabling management of complete platforms/IDPs</li> <li>Adding services to multiple managed clusters, enabling at-scale implementation and lifecycle management of standardized environments</li> <li>(As a Platform Architect) Authorizing cluster and service templates for use by others, and constraining their use within guardrails (enabling self-service)</li> <li>(As an authorized user) Leveraging shared cluster and service templates to lifecycle manage platforms (performing self-service)</li> <li>... and more</li> </ul> <p>Ready? Let's discover how to use k0rdent!</p>"},{"location":"k0rdent-architecture/","title":"k0rdent architecture","text":"<p>Text coming. Draft in GDocs. </p>"},{"location":"quickstart-1-mgmt-node-and-cluster/","title":"QuckStart 1 - Set up Management Node and Cluster","text":"<p>Please review the Guide to QuickStarts for preliminaries. This QuickStart unit details setting up a single-VM environment for managing and interacting with k0rdent, and for hosting k0rdent components on a single-node local Kubernetes management cluster. Once k0rdent is installed on the management cluster, you can drive k0rdent by SSHing into the management node (kubectl is there and will be provisioned with the appropriate kubeconfig) or remotely by various means (e.g., install the management cluster kubeconfig in Lens or another Kubernetes dashboard on your laptop, tunnel across from your own local kubectl, etc.)</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-a-single-node-k0s-cluster-locally-to-work-as-k0rdents-management-cluster","title":"Install a single-node k0s cluster locally to work as k0rdent's management cluster","text":"<p>k0s Kubernetes is a CNCF-certified minimal single-binary Kubernetes that installs with one command, and brings along its own CLI. We're using it to quickly set up a single-node management cluster on our manager node. However, k0rdent works on any CNCF-certified Kubernetes. If you choose to use something else, Team k0rdent would love to hear how you set things up to work for you.</p> <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh\nsudo k0s install controller --single\nsudo k0s start\n</code></pre> <p>You can check to see if the cluster is working by leveraging kubectl (installed and configured automatically by k0s) via the k0s CLI:</p> <pre><code>sudo k0s kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES    AGE   VERSION\nip-172-31-29-61   Ready    &lt;none&gt;   46s   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-kubectl","title":"Install kubectl","text":"<p>k0s installs a compatible kubectl and makes it accessible via its own client. But to make your environment easier to configure, we advise installing kubectl the normal way on the manager node and using it to control the local k0s management cluster.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list   # helps tools such as command-not-found to work correctly\nsudo apt-get update\nsudo apt-get install -y kubectl\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#get-the-local-k0s-clusters-kubeconfig-for-kubectl","title":"Get the local k0s cluster's kubeconfig for kubectl","text":"<p>On startup, k0s stores the administrator's kubeconfig in a local directory, making it easy to access:</p> <pre><code>sudo cp /var/lib/k0s/pki/admin.conf KUBECONFIG\nsudo chmod +r KUBECONFIG\nexport KUBECONFIG=./KUBECONFIG\n</code></pre> <p>At this point, your newly-installed kubectl should be able to interoperate with the k0s management cluster with administrative privileges. Test to see that the cluster is ready (usually takes about one minute):</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              STATUS   ROLES           AGE   VERSION\nip-172-31-29-61   Ready    control-plane   25m   v1.31.2+k0s\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-helm","title":"Install Helm","text":"<p>The Helm Kubernetes package manager is used to install k0rdent services. We'll install Helm as follows:</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n</code></pre> <p>Issuing these commands should produce something very much like the following output:</p> <pre><code>Downloading https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#install-k0rdent-into-the-k0s-management-cluster","title":"Install k0rdent into the k0s management cluster","text":"<p>Now we'll install k0rdent itself into the k0s management cluster:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre> <p>You'll see something like the following. Ignore the warnings, since this is an ephemeral, non-production, non-shared environment:</p> <pre><code>WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: ./KUBECONFIG\nWARNING: Kubernetes configuration file is world-readable. This is insecure. Location: ./KUBECONFIG\nPulled: ghcr.io/mirantis/hmc/charts/hmc:0.0.3\nDigest: sha256:1f75e8e55c44d10381d7b539454c63b751f9a2ec6c663e2ab118d34c5a21087f\nNAME: hmc\nLAST DEPLOYED: Mon Dec  9 00:32:14 2024\nNAMESPACE: hmc-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>k0rdent startup takes several minutes.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-k0rdent-cluster-management-pods-are-running","title":"Check that k0rdent cluster management pods are running","text":"<p>One fundamental k0rdent subsystem, k0rdent Cluster Manager (KCM), handles cluster lifecycle management on clouds and infrastructures: i.e., it helps you configure and compose clusters and manages infrastructure via Cluster API (CAPI). Before continuing, check that KCM pods are ready:</p> <pre><code>kubectl get pods -n kcm-system   # check pods in the kcm-system namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre> <p>Pods reported in states other than Running should become ready momentarily.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#check-that-the-sveltos-pods-are-running","title":"Check that the sveltos pods are running","text":"<p>The other fundamental k0rdent subsystem, k0rdent Service Manager (KSM), handles services configuration and lifecycle management on clusters. This utilizes the sveltos Kubernetes Add-On Controller and other open source projects. Before continuing, check that KSM pods are ready:</p> <pre><code>kubectl get pods -n projectsveltos   # check pods in the projectsveltos namespace\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre> <p>If you have fewer pods than shown above, just wait a little while for all the pods to reconcile and start running.</p>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-provider-and-related-templates-are-available","title":"Verify that KCM provider and related templates are available","text":"<p>k0rdent KCM leverages CAPI to manage Kubernetes cluster assembly and host infrastructure. CAPI requires infrastructure providers for different clouds and infrastructure types. These are delivered and referenced within k0rdent using templates, instantied in the management cluster as objects. Before continuing, verify that default provider template objects are installed and verified. Other templates are also stored as provider templates in this namespace \u2014 for example, the templates that determine setup of KCM itself and other parts of the k0rdent system (e.g., projectsveltos, which is a component of k0rdent Service Manager (KSM, see below)) as well as the k0smotron subsystem, which enables creation and lifecycle management of managed clusters that use Kubernetes-hosted control planes (i.e., control planes as pods):</p> <pre><code>kubectl get providertemplate -n kcm-system   # list providertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below. The placeholder X-Y-Z represents the current version number of the template, and will be replaced in the listing with digits:</p> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-kcm-clustertemplate-objects-are-available","title":"Verify that KCM ClusterTemplate objects are available","text":"<p>CAPI also requires control plane and bootstrap (worker node) providers to construct and/or manage different Kubernetes cluster distros and variants. Again, these providers are delivered and referenced within k0rdent using templates, instantied in the management cluster as <code>ClusterTemplate</code> objects. Before continuing, verify that default ClusterTemplate objects are installed and verified:</p> <pre><code>kubectl get clustertemplate -n kcm-system   # list clustertemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#verify-that-ksm-servicetemplate-objects-are-available","title":"Verify that KSM ServiceTemplate objects are available","text":"<p>k0rdent Service Manager (KSM) uses Service Templates to lifecycle manage services and applications installed on clusters. These, too, are represented as declarative templates, instantiated as ServiceTemplate objects. Check that default ServiceTemplate objects have been created and validated:</p> <pre><code>kubectl get servicetemplate -n kcm-system   # list servicetemplate objects in the kcm-system namespace\n</code></pre> <p>You should see output similar to what's shown below:</p> <pre><code>NAME                      VALID\ncert-manager-1-16-2       true\ndex-0-19-1                true\nexternal-secrets-0-11-0   true\ningress-nginx-4-11-0      true\ningress-nginx-4-11-3      true\nkyverno-3-2-6             true\nvelero-8-1-0              true\n</code></pre>"},{"location":"quickstart-1-mgmt-node-and-cluster/#next-steps","title":"Next steps","text":"<p>Your QuickStart management node is now complete, and k0rdent is installed and operational. Next, it's time to select AWS or Azure as an environment for hosting managed clusters.</p>"},{"location":"quickstart-2-aws/","title":"QuickStart 2 - AWS target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Amazon Web Services (AWS), and deploying our first managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to AWS account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our Azure QuickStart (QuickStart 2 - Azure target environment) you can continue here with steps to add the ability to manage clusters on AWS. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to AWS (for example, it could be on an Azure virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage AWS \u2014 doing so via the CAPA (ClusterAPI for AWS) provider. So a best practice for using k0rdent with AWS (this pattern is repeated with other clouds and infrastructures) is to create a new 'k0rdent user' on your account with the particular permissions k0rdent and CAPA require.</p> <p>In this section, we'll create and configure IAM for that user, and perform other steps to make that k0rdent user's credentials accessible to k0rdent in the management node. Note: if you're working on a shared AWS account, please ensure that the k0rdent user is not already set up before creating a new one.</p> <p>Creating a k0rdent user with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-aws/#install-the-aws-cli","title":"Install the AWS CLI","text":"<p>We'll use the AWS CLI to create and set IAM permissions for the k0rdent user, so we'll install it on our management node:</p> <pre><code>sudo apt install unzip\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \nunzip awscliv2.zip \nsudo ./aws/install\n</code></pre>"},{"location":"quickstart-2-aws/#install-clusterawsadm","title":"Install clusterawsadm","text":"<p>k0rdent uses Cluster API (CAPI) to marshal clouds and infrastructures. For AWS, this means using the components from the Cluster API Provider AWS (CAPA) project. This QuickStart leverages clusterawsadm, a CLI tool created by CAPA project that helps with AWS-specific tasks like IAM role, policy, and credential configuration.</p> <p>To install clusterawsadm on Ubuntu on x86 hardware:</p> <pre><code>curl -LO https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases/download/v2.7.1/clusterawsadm-linux-amd64\nsudo install -o root -g root -m 0755 clusterawsadm-linux-amd64 /usr/local/bin/clusterawsadm\n</code></pre>"},{"location":"quickstart-2-aws/#export-your-administrative-credentials","title":"Export your administrative credentials","text":"<p>You should have these already, preserved somewhere safe. If not, you can visit the AWS webUI (Access Management &gt; Users) and generate new credentials (Access Key ID, Secret Access Key, and Session Token (if using multi-factor authentication)).</p> <p>Export the credentials to the management node environment:</p> <pre><code>export AWS_REGION=EXAMPLE_AWS_REGION\nexport AWS_ACCESS_KEY_ID=EXAMPLE_ACCESS_KEY_ID\nexport AWS_SECRET_ACCESS_KEY=EXAMPLE_SECRET_ACCESS_KEY\nexport AWS_SESSION_TOKEN=EXAMPLE_SESSION_TOKEN # Optional. If you are using Multi-Factor Auth.\n</code></pre> <p>These credentials will be used both by the AWS CLI (to create your k0rdent user) and by clusterawsadm (to create a CloudFormation template used by CAPA within k0rdent).</p>"},{"location":"quickstart-2-aws/#create-the-k0rdent-aws-user","title":"Create the k0rdent AWS user","text":"<p>Now we can use the AWS CLI to create a new k0rdent user:</p> <pre><code> aws iam create-user --user-name k0rdentQuickstart\n</code></pre> <p>You'll see something like what's shown below. You should save this data securely. Note the Amazon Resource Name (ARN) because we'll be using it right away:</p> <pre><code>{\n    \"User\": {\n        \"Path\": \"/\",\n        \"UserName\": \"k0rdentQuickstart\",\n        \"UserId\": \"EXAMPLE_USER_ID\",\n        \"Arn\": \"arn:aws:iam::FAKE_ARN_123:user/k0rdentQuickstart\",\n        \"CreateDate\": \"2025-01-18T08:15:27+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart-2-aws/#attach-iam-policies-to-the-k0rdent-user","title":"Attach IAM policies to the k0rdent user","text":"<p>Next, we'll attach appropriate policies to the k0rdent user. These are:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> <p>We use the AWS CLI to attach them. To do this, you will need to extract the Amazon Resource Name (ARN) for the newly-created user. In the above example of content returned from the AWS CLI on user creation (see above) that's marked with the placeholder <code>FAKE_ARN_123</code>:</p> <p>Given this, you can assemble and execute the following commands to implement the required policies:</p> <pre><code>aws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\naws iam attach-user-policy --user-name k0rdentQuickstart --policy-arn arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\n</code></pre> <p>We can check to see that policies were assigned:</p> <p><pre><code>aws iam list-policies --scope Local\n</code></pre> And you'll see output that looks like this (this is non-valid example text):</p> <pre><code>{\n    \"Policies\": [\n        {\n            \"PolicyName\": \"controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF3VUDTMH3N\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers-eks.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNF5TAKL44PU\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/nodes.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:44+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:44+00:00\"\n        },\n        {\n            \"PolicyName\": \"controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFVO6OHIQOE\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/controllers.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 3,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        },\n        {\n            \"PolicyName\": \"control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"PolicyId\": \"ANPA22CF4NNFY4FJ3DA2E\",\n            \"Arn\": \"arn:aws:iam::FAKE_ARN_123:policy/control-plane.cluster-api-provider-aws.sigs.k8s.io\",\n            \"Path\": \"/\",\n            \"DefaultVersionId\": \"v1\",\n            \"AttachmentCount\": 2,\n            \"PermissionsBoundaryUsageCount\": 0,\n            \"IsAttachable\": true,\n            \"CreateDate\": \"2025-01-01T18:47:43+00:00\",\n            \"UpdateDate\": \"2025-01-01T18:47:43+00:00\"\n        }\n    ]\n}\n</code></pre>"},{"location":"quickstart-2-aws/#create-aws-credentials-for-the-k0rdent-user","title":"Create AWS credentials for the k0rdent user","text":"<p>In the AWS IAM Console, you can now create the Access Key ID and Secret Access Key for the k0rdent user and download them. You can also do this via the AWS CLI:</p> <pre><code>aws iam create-access-key --user-name k0rdentQuickstart\n</code></pre> <p>You should see something like this. It's important to save these credentials securely somewhere other than the management node, since the management node may end up being ephemeral. Again, this is non-valid example text:</p> <pre><code>{\n    \"AccessKey\": {\n        \"UserName\": \"k0rdentQuickstart\",\n        \"AccessKeyId\": \"EXAMPLE_ACCESS_KEY_ID\",\n        \"Status\": \"Active\",\n        \"SecretAccessKey\": \"EXAMPLE_SECRET_ACCESS_KEY\",\n        \"CreateDate\": \"2025-01-18T08:33:35+00:00\"\n    }\n}\n</code></pre>"},{"location":"quickstart-2-aws/#configure-aws-iam-for-k0rdent","title":"Configure AWS IAM for k0rdent","text":"<p>Before k0rdent CAPI can manage resources on AWS, you need to prepare for this by using <code>clusterawsadm</code> to create a bootstrap CloudFormation stack with additional IAM policies and a service account. You do this under the administrative account credentials you earlier exported to the management node environment:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre>"},{"location":"quickstart-2-aws/#create-iam-credentials-secret-on-the-management-cluster","title":"Create IAM credentials secret on the management cluster","text":"<p>Next, we create a secret containing credentials for the k0rdent user and apply this to the management cluster running k0rdent, in the kcm-system namespace (important: if you use another namespace, k0rdent will be unable to read the credentials). To do this, create the following YAML in a file called `aws-cluster-identity-secret.yaml':</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\ntype: Opaque\nstringData:\n  AccessKeyID: \"EXAMPLE_ACCESS_KEY_ID\"\n  SecretAccessKey: \"EXAMPLE_SECRET_ACCESS_KEY\"\n</code></pre> <p>Remember: the Access Key ID and Secret Access Key are the ones you generated for the k0rdent user, k0rdentQuickStart.</p> <p>Then apply this YAML to the management cluster as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-awsclusterstaticidentity-object","title":"Create the AWSClusterStaticIdentity object","text":"<p>Next, we need to create an <code>AWSClusterStaticIdentity</code> object that uses the secret.</p> <p>To do this, create a YAML file named <code>aws-cluster-identity</code> as follows:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Note that the <code>spec.secretRef</code> is the same as the <code>metadata.name</code> of the secret we just created.</p> <p>Create the object as follows:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml  -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#create-the-k0rdent-cluster-manager-credential-object","title":"Create the k0rdent Cluster Manager credential object","text":"<p>Now we create the k0rdent Cluster Manager credential object. As in immediately prior steps, create a YAML file called `aws-cluster-identity-cred.yaml':</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n</code></pre> <p>Note that .spec.identityRef.kind must be AWSClusterStaticIdentity and .spec.identityRef.name must match the .metadata.name of the AWSClusterStaticIdentity object.</p> <p>Now apply this YAML to your management cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml -n kcm-system\n</code></pre>"},{"location":"quickstart-2-aws/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage AWS. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>aws-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart-2-aws/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-aws-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under (that of your k0rdent user), plus the region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"quickstart-2-aws/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-aws-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-aws-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. You can watch the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>In a short while, you'll see output like what's below:</p> <pre><code>NAME                        READY   STATUS\nmy-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-aws/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                        READY   STATUS\nkcm-system   my-aws-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-aws/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-aws-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-aws-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart-2-aws/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"quickstart-2-azure/","title":"QuickStart 2 - Azure target environment","text":"<p>In this QuickStart unit, we'll be gathering information and performing preparatory steps to enable k0rdent (running on your management node) to manage clusters on Azure, and deploying a managed cluster.</p> <p>As noted in the Guide to QuickStarts, you'll need administrative access to an Azure account to complete this step. If you haven't yet created a management node and installed k0rdent, go back to QuickStart 1 - Management node and cluster.</p> <p>Note that if you have already done our AWS QuickStart (QuickStart 2 - AWS target environment) you can continue here with steps to add the ability to manage clusters on Azure. The k0rdent management cluster can accommodate multiple provider and credential setups, enabling management of multiple infrastructures. And even if your management node is external to Azure (for example, it could be on an AWS EC2 virtual machine), as long as you permit outbound traffic to all IP addresses from the management node, this should work fine. A big benefit of k0rdent is that it provides a single point of control and visibility across multiple clusters on multiple clouds and infrastructures.</p> <p>Cloud Security 101 Note: k0rdent requires some but not all permissions to manage Azure resources \u2014 doing so via the CAPZ (ClusterAPI for Azure) provider. So a best practice for using k0rdent with Azure (this pattern is repeated with other clouds and infrastructures) is to create a new k0rdent Azure Cluster Identity and Service Principal (SP) on your account with the particular permissions k0rdent and CAPZ require.</p> <p>In this section, we'll create and configure those identity abstractions, and perform other steps to make required credentials accessible to k0rdent in the management node. Note: if you're working on a shared Azure account, please ensure that the Azure Cluster Identity and Service Principal are not already set up before creating new abstractions.</p> <p>Creating user identity abstractions with minimal required permissions is one of several principle-of-least-privilege mechanisms used to help ensure security as organizations work with k0rdent at progressively greater scales. For more on k0rdent security best practices, please see the Administrator Guide.</p>"},{"location":"quickstart-2-azure/#install-the-azure-cli-az","title":"Install the Azure CLI (az)","text":"<p>The Azure CLI (az) is required to interact with Azure resources. Install it according to instructions in How to install the Azure CLI. For Linux/Debian (i.e., Ubuntu Server), it's one command:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre>"},{"location":"quickstart-2-azure/#log-in-with-azure-cli","title":"Log in with Azure CLI","text":"<p>Run the az login command to authenticate your session with Azure.</p> <pre><code>az login\n</code></pre>"},{"location":"quickstart-2-azure/#register-resource-providers","title":"Register resource providers","text":"<p>Azure Resource Manager uses resource providers to manage resources of all different kinds, and required providers must be registered with an Azure account before k0rdent and CAPZ can work with them.</p> <p>You can list resources registered with your account using Azure CLI:</p> <pre><code>az provider list --query \"[?registrationState=='Registered']\" --output table\n</code></pre> <p>And see a listing like this:</p> <pre><code>Namespace                             RegistrationState\n-----------------------------------   -----------------\nMicrosoft.Compute                     Registered\nMicrosoft.Network                     Registered\n</code></pre> <p>You can then select from the commands below (or enter all of them) to register any unregistered resources that k0rdent and CAPZ require:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre>"},{"location":"quickstart-2-azure/#get-your-azure-subscription-id","title":"Get your Azure Subscription ID","text":"<p>Use the following command to list Azure subscriptions and their IDs:</p> <pre><code>az account list -o table\n</code></pre> <p>The output will look like this:</p> <pre><code>Name                     SubscriptionId                    TenantId\n-----------------------  -------------------------------   -----------------------------\nMy Azure Subscription    SUBSCRIPTION_ID_SUBSCRIPTION_ID   TENANT_ID_TENANT_ID_TENANT_ID\n</code></pre> <p>The Subcription ID is in the second column.</p>"},{"location":"quickstart-2-azure/#create-a-service-principal-for-k0rdent","title":"Create a Service Principal for k0rdent","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure. To create it, run the following command with the Azure CLI, replacing  with the ID you copied earlier. <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You'll see output that resembles what's below:</p> <pre><code>{\n \"appId\": \"SP_APP_ID_SP_APP_ID\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"SP_PASSWORD_SP_PASSWORD\",\n \"tenant\": \"SP_TENANT_SP_TENANT\"\n}\n</code></pre> <p>Capture this output and secure the values it contains. We'll need several of these in a moment.</p>"},{"location":"quickstart-2-azure/#create-a-secret-object-with-the-sp-password","title":"Create a Secret object with the SP password","text":"<p>Now we'll create a new Secret to store the Service Principal password.</p> <p>Create a YAML file called <code>azure-cluster-identity-secret.yaml</code>, as follows, inserting the password for the Service Principal (represented by the placeholder <code>SP_PASSWORD_SP_PASSWORD</code> above):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\nstringData:\n  clientSecret: SP_PASSWORD_SP_PASSWORD # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Apply the YAML to the k0rdent management cluster using the following command:</p> <pre><code>kubectl apply -f azure-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quickstart-2-azure/#create-the-azureclusteridentity-object","title":"Create the AzureClusterIdentity Object","text":"<p>This object defines the credentials k0rdent and CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Create a YAML file called <code>azure-cluster-identity.yaml</code>. Make sure that <code>.spec.clientSecret.name</code> matches the <code>metadata.name</code> in the file you created above.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: SP_APP_ID_SP_APP_ID # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: SP_TENANT_SP_TENANT # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <p>```shell: kubectl apply -f azure-cluster-identity.yaml <pre><code>You should see output resembling this:\n\n```console\nazureclusteridentity.infrastructure.cluster.x-k8s.io/azure-cluster-identity created\n</code></pre></p>"},{"location":"quickstart-2-azure/#create-the-kcm-credential-object","title":"Create the KCM Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-cluster-identity-cred.yaml</code>.</p> <p>Note that <code>.spec.kind</code> must be <code>AzureClusterIdentity</code> and <code>.spec.name</code> must match <code>.metadata.name</code> of the AzureClusterIdentity object created in the previous step.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity-cred.yaml\n</code></pre> <p>You should see output resembling this:</p> <pre><code>credential.k0rdent.mirantis.com/azure-cluster-identity-cred created\n</code></pre>"},{"location":"quickstart-2-azure/#find-your-locationregion","title":"Find your location/region","text":"<p>To determine where to deploy your cluster, you may wish to begin by listing your Azure location/regions:</p> <pre><code>az account list-locations -o table\n</code></pre> <p>You'll see output like this:</p> <pre><code>DisplayName               Name                 RegionalDisplayName\n------------------------  -------------------  -------------------------------------\nEast US                   eastus               (US) East US\nSouth Central US          southcentralus       (US) South Central US\nWest US 2                 westus2              (US) West US 2\nWest US 3                 westus3              (US) West US 3\nAustralia East            australiaeast        (Asia Pacific) Australia East\n</code></pre> <p>What you'll need to insert in your ClusterDeployment is the name (center column) of the region you wish to deploy to.</p>"},{"location":"quickstart-2-azure/#list-available-cluster-templates","title":"List available cluster templates","text":"<p>k0rdent is now fully configured to manage Azure. To create a cluster, begin by listing the available ClusterTemplates provided with k0rdent:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>You'll see output resembling what's below. Grab the name of the AWS standalone cluster template in its present version (in the below example, that's <code>azure-standalone-cp-0-0-5</code>):</p> <pre><code>NAMESPACE    NAME                            VALID\nkcm-system   adopted-cluster-0-0-2           true\nkcm-system   aws-eks-0-0-3                   true\nkcm-system   aws-hosted-cp-0-0-4             true\nkcm-system   aws-standalone-cp-0-0-5         true\nkcm-system   azure-aks-0-0-2                 true\nkcm-system   azure-hosted-cp-0-0-4           true\nkcm-system   azure-standalone-cp-0-0-5       true\nkcm-system   openstack-standalone-cp-0-0-2   true\nkcm-system   vsphere-hosted-cp-0-0-5         true\nkcm-system   vsphere-standalone-cp-0-0-5     true\n</code></pre>"},{"location":"quickstart-2-azure/#create-your-clusterdeployment","title":"Create your ClusterDeployment","text":"<p>Now, to deploy a cluster, create a YAML file called <code>my-azure-clusterdeployment1.yaml</code>. We'll use this to create a ClusterDeployment object in k0rdent, representing the deployed cluster. The ClusterDeployment identifies for k0rdent the ClusterTemplate you wish to use for cluster creation, the identity credential object you wish to create it under, plus the location/region and instance types you want to use to host control plane and worker nodes:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5 # name of the clustertemplate\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"AZURE_LOCATION\" # Select your desired Azure Location\n    subscriptionID: SUBSCRIPTION_ID_SUBSCRIPTION_ID # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre>"},{"location":"quickstart-2-azure/#apply-the-clusterdeployment-to-deploy-the-cluster","title":"Apply the ClusterDeployment to deploy the cluster","text":"<p>Finally, we'll apply the ClusterDeployment YAML (<code>my-azure-clusterdeployment1.yaml</code>) to instruct k0rdent to deploy the cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>Kubernetes should confirm this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com/my-azure-clusterdeployment1 created\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre>"},{"location":"quickstart-2-azure/#obtain-the-clusters-kubeconfig","title":"Obtain the cluster's kubeconfig","text":"<p>Now you can retrieve the cluster's kubeconfig:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <p>And you can use the kubeconfig to see what's running on the cluster:</p> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\"\nkubectl get pods -A\n</code></pre>"},{"location":"quickstart-2-azure/#list-managed-clusters","title":"List managed clusters","text":"<p>To verify the presence of the managed cluster, list the available ClusterDeployments:</p> <pre><code>kubectl get ClusterDeployments -A\n</code></pre> <p>You'll see output something like this:</p> <pre><code>NAMESPACE    NAME                          READY   STATUS\nkcm-system   my-azure-clusterdeployment1   True    ClusterDeployment is ready\n</code></pre>"},{"location":"quickstart-2-azure/#tear-down-the-managed-cluster","title":"Tear down the managed cluster","text":"<p>To tear down the managed cluster, delete the ClusterDeployment:</p> <pre><code>kubectl delete ClusterDeployment my-azure-clusterdeployment1 -n kcm-system\n</code></pre> <p>You'll see confirmation like this:</p> <pre><code>clusterdeployment.k0rdent.mirantis.com \"my-azure-clusterdeployment1\" deleted\n</code></pre>"},{"location":"quickstart-2-azure/#next-steps","title":"Next Steps","text":"<p>Now that you've finished the k0rdent QuickStart, we have some suggestions for what to do next:</p> <p>Check out the Administrator Guide ...</p> <ul> <li>For a more detailed view of k0rdent setup for production</li> <li>For details about setting up k0rdent to manage clusters on VMware and OpenStack</li> <li>For details about using k0rdent with cloud Kubernetes distros: AWS EKS and Azure AKS</li> </ul> <p>Or check out the Demos Repository for fast, makefile-driven demos of k0rdent's key features!</p>"},{"location":"why-k0rdent/","title":"Why k0rdent?","text":"<p>With the increasing complexity of modern systems, it has become more and more necessary to provide a way for administrators to manage that complexity while still providing the means for developers to get their jobs done as efficiently as possible.  Enter platforms. These frameworks or environments provide the foundation and tools for developing, deploying, and managing applications, services, or systems, enabling users to focus on their specific tasks or goals while abstracting away the underlying complexities.</p> <p>As the need for platform engineering, the practice of designing, building, and maintaining these platforms, becomes evident, so do some of the challenges of creating a truly functional platform engineering environment. Assembling, integrating, deploying, and managing Kubernetes clusters is straightforward in theory, but where companies may have once had a single multi-purpose platform, the current trend is to implement many more purpose-dedicated, simpler platforms. While this makes managing each platform easier, you\u2019re left with a sum that\u2019s more than its parts, both in capability and complexity. Once you add in multiple clouds, differing infrastructures, and the need to prepare those infrastructures to host workloads, platform engineering moves from being a task on an administrator\u2019s plate to being an entire discipline.</p> <p>The solution must be answerable to real world needs. This moves beyond simple resilience and availability to awareness of security and compliance requirements, including access management and sharing. Platform engineers need to be able to compose platforms\u2013for example, Kubernetes, beach-head services such as CI/CD, and any other dependencies workloads require\u2013in simple, repeatable ways. They also need to be able to safely and flexibly share access and artifacts with platform leads and other platform 'consumers/users.' On top of that, platforms must enable visibility into both the infrastructure and applications, including both performance and cost monitoring and ensuring platform integrity with state monitoring and continuous reconciliation, all while simplifying platform operations for Day 2 and beyond.</p> <p>All of these functionalities are available in open source. Kubernetes is the most obvious substrate, of course, providing tools for not only container orchestration, but also custom operators. Fortunately, there are optimized distros of Kubernetes such as k0s, and Kubernetes itself can be orchestrated using the Kubernetes Cluster API (CAPI), which can both manage clouds and infrastructure and assemble, create and lifecycle manage clusters on those infrastructures. Add open source tools such as Helm and other package managers to help manage deployment, upgrade, and removal of applications, and you have a complete platform engineering solution.</p> <p>Of course, getting all of the relevant pieces to work together\u2013particularly over multiple infrastructures\u2013isn\u2019t simple or straightforward. That\u2019s why you need k0rdent. k0rdent puts all of these pieces together so you don\u2019t have to.</p>"}]}